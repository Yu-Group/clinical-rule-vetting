---
title: 'Predicting Low Risk of Clinically-Important TBI in Children: Vetting and Improving Existing Rules'
author: Jimmy Butler, Andrej Leban, Ian Shen, Xin Zhou
output: pdf_document
bibliography: references.bib
---

# Introduction and Problem Motivation

Among children ages 0-14, traumatic brain injuries (TBIs) are a leading cause of injury and death,
accounting for over 2100 deaths, 35,000 hospital admissions, and 470,000 emergency
department visits annually (Faul, Xu, Wald, & Coronado, 2010). Accurately diagnosing TBIs is thus a crucial
problem, especially since the earlier a TBI is identified, the sooner potentially life-saving
treatment can be administered. 

Typically, TBIs are diagnosed via computed tomography (CT)
scans of a patient's brain. However, performing CT scans carries the risk of radiation-induced malignancy in the patient.
This risk is especially pertinent for children, as younger individuals are at a higher
risk of developing such malignancies. Therefore, while it is crucial to rapidly and
accurately diagnose TBIs, it is also imperative that CT scans are performed only when
there is reasonable suspicion the individual has a TBI. Otherwise, the risk of
radiation-induced complications from the CT scan will outweigh any potential benefit
of having CT scan results.

Nathan Kuppermann and his colleagues explored this problem in their 2009 Lancet paper
("Identification of children at very low risk of clinically-important traumatic brain injuries after
head trauma: a prospective cohort study"), seeking to develop interpretable rules
that could be used to identify children, who, despite presenting to the emergency room
for head trauma, are at low risk of having TBIs and thus can avoid CT scans. The purpose
of this present study is to build on their work, developing alternative classification
schemes and rule sets to identify children for whom CT scans are unnecessary. With an
alternative scheme with high predictive accuracy, one can compare with Kuppermann's to see
if similar rules were isolated, either reinforcing the importance of their identified covariates,
or providing a new perspective on what is important in predicting TBI. Additionally,
in this analysis, we employ our own data cleaning and preprocessing scheme that we find
reasonable, allowing us to assess the stability of their findings subject to perturbations
at this stage of the data science life cycle.

In addition to vetting their classification rules, another goal of this analysis
is to offer some improvement on their classification strategy. Kuppermann and his
colleagues developed rule sets for children aged two or younger and children
older than two, the reason being that younger children are pre-verbal and thus require
a different set of standards to identify TBIs than older children who can verbalize
their symptoms (citation). We seek to develop a single age-invariant
classifier using covariates common to both primary age groups. We hope that such
a classifier will not only identify common predictors of TBI across all ages, but also
simplify the decision-making process by obviating the need for a second classifier.
Nonetheless, in developing new classifiers, priority will be placed on interpretability, 
brevity and simplicity, and a low false negative rate. 

# Data

## Data Collection

Before going into extensive detail about the dataset used in this analysis, as
well as our cleaning and preprocessing steps, we first offer our own thoughts
on what data would be most relevant for vetting previous classifiers and developing
an age-invariant classifier. 

Besides needing a dataset similar in nature
to the dataset on which the original classifiers were based (comprised of children
presenting to ER with head trauma, etc.), to vet the previous
classifiers, we need a dataset that will enable us to develop classifiers with
high predictive accuracy and maximal generalizability so as to compare with previous classifiers. 
For instance, we would need a dataset containing covariates that are both relevant to the presence
of TBIs and can be measured at the time a child is ushered into the ER. From common
sense, it seems important information might be the injury mechanism (and a rating of
severity), any symptoms such as headache or amnesia, and any results of a physical
examination that may indicate neurological deficits or skull injury. For both
vetting the clinical decision rule and developing an age-invariant rule, it would
be preferable to have covariates that are as objective as possible so as to avoid
issues with interrater reliability and improve generalizability of the rules to 
new, real-world settings. It would also be preferable to both have details on covariates 
invariant to gender, race, ethnicity, or any other such characteristic
of the population of interest and a population with gender, age, and racial diversity,
to reduce the potential of systematic misdiagnoses in these groups
should the rule be applied in future settings. Finally, we wish to identify children
who do not need CT scans among a population where there is already some doubt
as to whether the CT scan will be worth the risk of radiation induced malignancy.
So, the ideal dataset will consist of children who are not critically injured
and who could conceivably be TBI-free.

To develop an age-invariant classifier, the above points are certainly relevant.
However, specific to an age-invariant classifier, we would also need covariates
whose presence or values are not age-dependent.

The dataset used in this analysis is the same one used by Kuppermann and his colleagues
to develop their classification rules, consisting of 43399 children who presented
to the emergency room for head trauma at one of the PECARN network hospitals
between 2004 and 2006. For each
child, the data were collected by physicians who filled out standardized sheets detailing the
child's condition, including both questions for the child ('Do you have a headache?')
and results of a physician examination ('Palpable skull fracture?'). Additionally,
the dataset contains information that would be available after the decision to
pursue a CT scan has been made, such as the reasons why a CT scan is being pursued,
whether a child will need to be sedated for a CT scan, and whether there were 
any traumatic findings on the scan. The outcome variable (TBI) was obtained by
tracking the child's progress in the hospital after the initial examination 
(whether they were admitted for neurological surgery, or died in the ED, for example)
or, if the child left the hospital, following-up with the child's parents, the medical record,
emergency department process improvement records, and county morgue records to ensure
no TBI diagnoses were missed. In total, there are 125 covariates, including the outcome.

It seems that in this dataset, most of the variables we conjecture are important
for predicting TBIs are present, but that there are many covariates whose information
would not be known at the time our classifier would be implemented. Additionally,
many covariates seem more objective than others ('Palpable skull fracture?' vs. 
'Is the child acting normally?'). There are also many variables which are not invariant
to age. For instance, the ability to say you have a headache is reserved for verbal
children who must be above a certain age. These variables and decisions on what to do
with them will be discussed at length in Cleaning and Preprocessing, as well as
Post-Processing EDA. The dataset also provides demographic characteristics such as
age, gender, race, and ethnicity. This information will be discussed at length in 
Post-Processing EDA.

## Cleaning and Preprocessing

In order to achieve a suitable dataset for the predicting low risk of TBI
in children according to the above parameters, we first implement a set of
cleaning and preprocessing steps. The overarching goal of our cleaning
and preprocessing steps is to keep only covariates relevant to predicting TBI
at the time a child would present to the ER, keep only observations for whom
this classification rule would be relevant (children for whom it's a toss-up
as to whether they'll need a CT scan), and to deal with missing values. We present
our procedure below, along with relevant judgement calls and their perturbations
where applicable.

* Remove columns irrelevant to predicting TBI at the time a child enters the emergency room
  + *EmplType*, *Certification*, *Ind...*, *CT...*, *Finding1*, ... *Finding23*, *EDDisposition*, *Observed*, *AgeTwoPlus*, *AgeInMonth*
  + Judgement Call [`injMech`]: drop *InjuryMech* 
    + Perturbation: keep *InjuryMech* 

This includes *EmplType* and *Certification*, any variable about the reasons why a CT scan was ordered, whether sedation would be needed when performing a CT scan, the traumatic findings from the CT scan, the reason why the child left the ER, or 
whether the child was observed again to see if they needed a CT scan. 
We remove the former two variables because domain knowledge
says that we should not expect the qualifications of the physician to systematically
affect the responses collected. The rest of the variables would either not be available
at the time a child first enters the ER, or would be known only after
the decision to obtain a CT scan has been made. So, these are not of interest.
We also remove an indicator for the individuals being above age 2, as well as
the age provided in months, opting to keep the age provided in years. Similarly,
according to judgement call [`injMech`], we drop the injury mechanism, a 13-level categorical variable detailing how the
injury occurred, opting to keep a severity variable, a 3-tiered categorical variable
rating the severity of the injury mechanism as 'low', 'moderate', or 'high'.

* Remove columns with high levels of missingness
  + *Dizzy*, *Ethnicity*

*Dizzy*, an indicator of whether the individual felt dizzy, and *Ethnicity*,
whether the individual was Hispanic, are both missing in approximately 35% of the
observations. *Ethnicity* would be useful for posthoc analysis, but with such a large
number of missing values, it may not be particularly informative. *Dizzy* could be
a good indicator of TBI, but was deemed too subjective and prone to inter-rater reliability
issues according to both Kupperman et al. and Dr. Inglis.

* Remove observations whose *GCS* scores are less than 14

GCS, or Glasgow Coma Score, measures an individual's level of consciousness on a scale from 3 to 15,
and is the sum total of three subscores: an eye score, a verbal score, and a motor score.
If GCS is less than 14, the individual is quite injured and would likely need a CT scan anyway.
We only keep individuals where the total GCS is 14 or 15.

* Impute and Drop based on GCS scores
  + Judgement Call [`missSubGCS`]: drop observations with total GCS of 14, but missing any of the GCS subcategory scores (*GCSVerbal*, *GCSMotor*, *GCSEye*) 
    + Perturbation: keep the observations
  + Impute a full score into each GCS subcategory when total GCS is 15 
  + Judgement Call [`fake15GCS`]: drop observations with total GCS of 15, but suboptimal score in a subcategory 
    + Perturbation: keep the observations
  + Judgement Call [`fake14GCS`]: drop observations with total GCS of 14, but optimal score in every subcategory 
    + Perturbation: keep the observations
  
GCS score is likely an important predictor of TBI, so it is imperative this variable is as completely filled-out as possible in this dataset. However, it should be noted that
there are different standards for obtaining GCS scores for individuals younger than two
and older than two, since individuals younger than two cannot verbalize. We note this
as a potential problem to developing an age-invariant classifier.

* Remove observations who were pharmacologically paralyzed, sedated, or intubated at the time of evaluation, or for whom any of this information is missing
  + *Paralyzed*, *Sedated*, *Intubated*
  
According to domain knowledge, individuals who must be pharmacologically paralyzed, 
sedated, or intubated at the time of evaluation are likely quite injured and do not 
represent the population of interest for these classification rules.

* Drop and impute based on variables of altered mental status
  + *AMS*, *AMSAgitated*, *AMSSleep*, *AMSSlow*, *AMSRepeat*, *AMSOth*
  + Judgement Call [`AMS`]: drop observations missing any of the above variables
    + Perturbation: impute missing *AMS* based on presence of any of the subvariables, and drop the subvariables and any remaining missing *AMS* observations
    
*AMS* is an indicator representing if the child has altered mental status, and
the remaining variables are subvariables indicating the reason why the child has
altered mental status (agitated, sleepy, slow to respond, asking repetitive questions, etc.)
This is likely an important predictor, so it is important to handle missing values.
By judgement call `AMS`, we drop any observation with a missing value in any of these
categories, since if the observation is missing it is impossible to truly know a value
to impute for *AMS*. As a perturbation, we could impute a 1 in *AMS* if any
if any of the subcategories is affirmative, and 0 otherwise. However, it would
be impossible to impute for missing values in the subcategories, so we drop
these observations.

* Drop and impute based on variables of other non-head substantial injuries
  + *OSI*, *OSIExtremity*, *OSICut*, *OSICspine*, *OSIFlank*, *OSIAbdomen*, *OSIPelvis*, *OSIOth*
  + Judgement Call [`OSI`]: same procedure as above for *AMS* and its subvariables
    + Perturbation: same procedure as above for *AMS* and its subvariables
    
*OSI* is an indicator on whether the physician determined the child sustained 
substantial non-head injuries. This could be an injury to the extremities like a fracture,
a cut requiring surgery, a spinal cord injury, a flank injury, an intra-abdominal injury,
or a pelvis injury. Although not directly related to head trauma, we find this variable
might be an indicator of the injury severity. If another variable relevant to head trauma
fails to indicate a TBI for a particular individual, including this variable offers
another opportunity to diagnose the TBI, possibly suppressing the false negative rate.
By judgement call `OSI`, we drop any observations missing any of these values or their
subvariables for the same reason as judgement call `AMS`. 

* Drop and impute based on hematoma information
  + *Hema*, *HemaSize*, *HemaLoc*
  + Judgement Call [`HEMA`]: same procedure as above for *OSI* and its subvariables
    + Perturbation 1: same procedure as above for *OSI* and its subvariables
    + Perturbation 2: Only drop missing *HemaLoc* and *Hema*, impute missing *Hema* from *HemaLoc* presence, drop remaining missing *Hema*
    
*Hema* indicates the presence of a scalp hematoma, or swelling due to some head trauma,
*HemaSize* is a categorical variable representing the size (< 1cm, 1-3cm, > 3cm) and
*HemaLoc* is a categorical variable representing the location (frontal, occipital, parietal/temporal).
According to domain knowledge, scalp hematomas are fairly good predictors of TBI.
By judgement call [`HEMA`], we drop all observations missing any of these categories.
By Perturbation 1, we only keep the indicators for the presence of hematoma. By Perturbation 2, we keep the indicator as well as the location, since according to domain knowledge,
the impact of head trauma to the brain is dependent on the location of the trauma.

* Drop and impute based on skull fracture variables
  + *SFxPalp*, *SFxPalpDepress*
  + Judgement Call [`SFx`]: recode an unclear *SFxPalp* observation as true, drop observations missing either *SFxPalp* or *SFxPalpDepress*
    + Perturbation 1: same procedure as [`AMS`] perturbation
    + Perturbation 2: same procedure as [`AMS`] judgement call
    
*SFxPalp* is an indicator for a palpable skull fracture, which can also be marked as
'unclear' if there is too much swelling to make a determination. *SFxPalpDepress*
is an indicator on whether the fracture feels depressed. Both of these are likely
important predictors via domain knowledge. As a judgement call, we take an unclear
observation as being a palpable skull fracture, since even if the examination was unclear,
there is still substantial head trauma.

* Drop missing values for bulging Anterior Fontanelle
  + *FontBulg*

*FontBulg* is an indicator for whether the child's anterior fontanelle is bulging,
indicating pressure within the brain. This feature would typically only be present in
infants, since for older individuals their anterior fontanelle will have closed as their
skull hardens with age. While possibly a good predictor for TBI in infants, this
variable does not have much predictive value for older children.

* Drop and impute based on basilar skull fracture variables
  + *SFxBas*, *SFxBasHem*, *SFxBasOto*, *SFxBasPer*, *SFxBasRet*, *SFxBasRhi*
  + Judgement Call [`SFxBas`]: same as [`AMS`] judgement call
    + Perturbation: same as [`AMS`] perturbation

*SFxBas* is an indicator if the physician has determined there is basilar skull fracture
present, or a fracture at the base of the skull. The rest of the variables indicate
possible signs that go along with basilar skull fracture (hemotympanum, otorrhea, raccoon eyes, Battle's sign, rhinorrhea). This is known to be quite an important predictor of
TBI, so we wish to make these variables as complete as possible for an ideal dataset.
To do so, we follow the same procedure for *AMS* and its subvariables, regarding missingness.

* Drop and impute based on above-the-clavicle injury variables
  + *Clav*, *ClavNeck*, *ClavFro*, *ClavOcc*, *ClavPar*, *ClavTem*
  + Judgement Call [`Clav`]: same as [`AMS`] judgement call
    + Perturbation: same as [`AMS`] perturbation

*Clav* is an indicator on whether there was a laceration, abrasion, or hematoma
above the clavicles (neck). The subvariables indicate whether there was trauma on
the neck or the frontal, occipital, parietal or temporal regions of the scalp.
In the same way as *OSI*, this variable will be useful to keep around because it
could indicate a TBI if one of the head trauma variables fails to.

* Drop and impute based on neurological deficit variables
  + *NeuroD*, *NeuroDMotor*, *NeuroDSensory*, *NeuroDCranial*, *NeuroDReflex*, *NeuroDOth*
  + Judgement Call [`Neuro`]: same as [`AMS`] judgement call
    + Perturbation: same as [`AMS`] perturbation
    
*NeuroD* is an indicator for whether or not there is some neurological deficit
besides altered mental status, as determined by the physician. This includes
motor, sensory, cranial nerve (pupil reactivity), or reflex issues. Generally,
these categories are quite important for predicting TBI according to domain knowledge.
As with previous groups of variables, we implement a judgement call to drop
any observation missing any of these values.

* Drop and impute based on vomiting variables
  + *Vomit*, *VomitNbr*, *VomitStart*, *VomitLast*
  + Judgement Call [`Vomit`]: same as [`AMS`] perturbation
    + Perturbation: same as [`AMS`] judgement call

*Vomit* is an indicator for whether the individual vomited (as reported by child
or parents), *VomitNbr* is the number of times the child vomited post-injury (1, 2, > 2 times),
*VomitStart* is how soon after the head injury the vomiting started (within 1 hour, 1 - 4 hours, > 4 hours after), and *VomitLast* is how soon before the evaluation was the last
episode (< 1 hour, 1 - 4 hours, > 4 hours before evaluation). As a judgement call,
we only keep *Vomit* and impute missing values from the subvariables, since according
to domain knowledge, more details about vomiting is generall unhelpful. As a perturbation,
we keep these covariates.

* Drop and impute based on headache variables
  + *HA_verb*, *HAStart*, *HASeverity*
  + Judgement Call [`HA`]: Drop missing *HASeverity*, impute missing *HA_verb* from *HASeverity*, and drop remaining missing *HA_verb*, ignore *HAStart* column 
    + Perturbation 1: same as [`AMS`] perturbation
    + Perturbation 2: same as [`AMS`] judgement call
    
*HA_verb* is an indiactor on whether the child had a headache, and is marked differently
if the child is too young to verbalize that they had a headache, or unable to speak.
*HAStart* and *HASeverity* are both variables indicating when the headache began
(before injury, within 1 hour after, 1-4 hours after, > 4 hours after) and the severity
(mild, moderate, and severe). As a judgement call, we keep *HA_verb* and *HASeverity*
since these are both more likely to predict TBI than *HAStart*. However, it should be
noted that this is an age-dependent variable as discussed in the prior section, since
being able to verbalize headache symptoms requires the child to be of a certain age.

* Drop and impute based on seizure variables
  + *Seiz*, *SeizLen*, *SeizOccur*
  + Judgement Call [`Seiz`]: Drop missing *SeizLen*, impute missing *Seiz* from *SeizLen*, drop missing *Seiz*, and ignore *SeizOccur* column
  
*Seiz* is an indicator if the child had a seizure, as reported by the parent or child.
*SeizLen* is how long the seizure lasted (< 1 min, 1-5 min, 5-15 min, >15 min) and
*SeizOccur* is when did it occur relative to the injury time (immediately on contact, 
within 30 minutes, or > 30 minutes after injury). Whether a seizure occured is 
important to consider, and the length is also somewhat important. So, we opt to
keep *SeizLen* and *Seiz*, but drop *SeizOccur*, since this is not as important
according to domain knowledge. 

* Drop and impute based on loss of consciousness variables
  + *LOCSeparate*, *LocLen*
  + Judgement Call [`LOC`]: recode 'suspected' as 'yes' in *LOCSeparate*, and drop observations with missing *LOCSeparate* and *LOCLen*
    + Perturbation 1: same as [`AMS`] perturbation
    + Perturbation 2: same as [`AMS`] judgement call
    
*LOCSeparate* is an indicator if the child lost consciousness at all after the injury,
and *LocLen* is how long the child lost consciousness (< 5 sec, 5 sec - 1 min, 1 - 5 min, > 5 min). Loss of consciousness is generally a good predictor of head trauma, so we wish
to include this to predict TBI. As a judgement call, we consider a suspected loss of
consciousness as affirmative, since if there is any doubt as to whether the child
lost consciousness, this should be of concern to err on the side of caution.

* Drop missing values for *Amnesia* and *High_impact_InjSev*

*Amensia* indicates whether the child has amnesia for the event, but can only
be answered by verbal children. This is an example of a variable that is not age-invariant,
although it could be a good predictor for verbal children. *High_impact_InjSev*
is the rating of the injury mechanism severity, as mentioned above. 

* Impute missing values for *ActNorm*
  + Judgement Call [`ActNorm`]: recode null values as affirmative
    + Perturbation: recode values as not affirmative

*ActNorm* is a subjective assessment by the parent as to whether the parent
thinks the child is acting normally. Although subjective, domain experts conclude
that a parents assessement of their child's behavior is a good indicator of
how severely the child may be injured. Because this is important, we wish to impute
as many values as possible. As a judgement call, we consider a missing value
as being normal, since if the child was acting normally, it's possible this
question could have simply been skipped over. As a perturbation, we reverse this
judgement call.

* Make a pooled outcome variable

We define the outcome, a clinically-important TBI, in the same way as Kuppermann
and his colleagues, except we impute missing values in the outcome by the
presence of any of the following: death due to TBI, intubated for more than 24 hours,
neurosurgery was performed, and the individual was hospitalized for more than 2 nights
due to head injury. This should give a complete assessment as to whether an individual
had a TBI.

## Data Summary

Once we have cleaned the data according to the above steps, we wish to extract
binary features from the categorical variables, where applicable. For every subvariable/
parent variable pairing mentioned above (for example, `NeuroDMotor` and `NeuroD`),
if the parent variable is marked as false, the subvariable has a value indicating
it is missing. As a default, we maintain this missing category, and binarize the variables
by one-hot encoding. Alternatively, we could impute false into the missing values
and drop the parent variables, so now each subvariable indicates the presence of the
subvariable AND the parent variable as opposed to the subvariable GIVEN the parent variable.

After performing this one-hot encoding, we have 137 features in total. We split 
the data into a training set (19947 observations), a validation set (6649 observations), 
and a test set (6649 observations). For post-processing EDA, we only consider
the training set, only using the test set to check model accuracy.

## Post-Processing EDA



Make sure you talk a bit about Relevance based on the EDA here.

This takes care of Q7

Show some post-processing EDA plots and descriptions of the data (cleaned according to canonical steps)

Currently, I'm thinking:

1. Show general plots of counts for different categories, etc.
2. Show plots of correlations between variables and with the outcome for just the umbrella variables
  -to justify only using the umbrella variables in our classifier
  -also possibly make some plots of conditioning on one umbrella variable and finding correlations between outcomes
  to justify the rules/sequences of decisions
3. Show plots of correlations between variables and with the outcome for the umbrella subcategories
  -to justify possibly including the subcategories as well, may provide different information
  -depends on what the domain expert says, though

Conclude with our feature extraction judgement calls: we will either include only the umbrella variables,
or all of them including subcategories, or just collapse the umbrellas into the subcategories (not sure how this last one will fit in, but we can see)

## Post-Processing Discussion

Answer Q4, Q5, Q6 here, and make sure the answers connect with what predictor/model
we will end up choosing.

# Models

Short sentence or two about how we will fit decision trees/rules for maximal interpretability

## Baseline Model

Short sentence or two about how we will be doing modelling

### Description

Describe the baseline classifier from the paper

### Procedure and Performance

Give the performance of the baseline model on the original data

## Proposed Model

Describe the proposed model, it's advantages, disadvantages, how it might compare
to the original model, etc.

Justify the proposed model, and refer back to discussion in Post-Processing Discussion
for justification whila also answering Q8 about any randomness assumptions. 

### Procedure and Performance

Describe the procedure for training the model with justification
Give the performance of the model

### Stability Analysis

Answer Question 9

Go through the perturbations of our judgement calls (which will have been introduced above)
and compare the performance of our classifier, see if it is similar or different

Also, if there was instability in training the classifiers (as Xin noted in his notebook
in our 12/2 meeting), run through it here as well

# Discussion

Discuss the performance, the results, the stability subject to perturbations,
how well it compared to the baseline, etc.

Discuss future directions on what we can do (other models, other perturbations,
things we missed that we would have done with more time, etc.)

# Conclusion

Just wrap it up

