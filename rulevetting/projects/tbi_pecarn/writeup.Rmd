---
title: 'Predicting Low Risk of Clinically-Important TBI in Children: Vetting and Improving Existing Rules'
author: Jimmy Butler, Andrej Leban, Ian Shen, Xin Zhou
output: pdf_document
bibliography: references.bib
---

```{r setup, echo = FALSE}
library(knitr)

library(reticulate)
reticulate::use_python("../../../rule-env/bin/python3", required = TRUE)
reticulate::use_virtualenv("../../../rule-env/", required = TRUE)
knitr::knit_engines$set(python = reticulate::eng_python)
knitr::opts_chunk$set(engine.path = list(python = "../../../rule-env/bin/python3"))
```

# Introduction and Problem Motivation

Among children ages 0-14, traumatic brain injuries (TBIs) are a leading cause of injury and death,
accounting for over 2100 deaths, 35,000 hospital admissions, and 470,000 emergency
department visits annually (Faul, Xu, Wald, & Coronado, 2010). Accurately diagnosing TBIs is thus a crucial
problem, especially since the earlier a TBI is identified, the sooner potentially life-saving
treatment can be administered. 

Typically, TBIs are diagnosed via computed tomography (CT)
scans of a patient's brain. However, performing CT scans carries the risk of radiation-induced malignancy in the patient.
This risk is especially pertinent for children, as younger individuals are at a higher
risk of developing such malignancies. Therefore, while it is crucial to rapidly and
accurately diagnose TBIs, it is also imperative that CT scans are performed only when
there is reasonable suspicion the individual has a TBI. Otherwise, the risk of
radiation-induced complications from the CT scan will outweigh any potential benefit
of having CT scan results.

Nathan Kuppermann and his colleagues explored this problem in their 2009 Lancet paper
("Identification of children at very low risk of clinically-important traumatic brain injuries after
head trauma: a prospective cohort study"), seeking to develop interpretable rules
that could be used to identify children, who, despite presenting to the emergency room
for head trauma, are at low risk of having TBIs and thus can avoid CT scans. The purpose
of this present study is to build on their work, developing alternative classification
schemes and rule sets to identify children for whom CT scans are unnecessary. With an
alternative scheme with high predictive accuracy, one can compare with Kuppermann's to see
if similar rules were isolated, either reinforcing the importance of their identified covariates,
or providing a new perspective on what is important in predicting TBI. Additionally,
in this analysis, we employ our own data cleaning and preprocessing scheme that we find
reasonable, allowing us to assess the stability of their findings subject to perturbations
at this stage of the data science life cycle.

In addition to vetting their classification rules, another goal of this analysis
is to offer some improvement on their classification strategy. Kuppermann and his
colleagues developed rule sets for children aged younger than two and children
aged two or older, the reason being that younger children are pre-verbal and thus require
a different set of standards to identify TBIs than older children who can verbalize
their symptoms (citation). We seek to develop a single age-invariant
classifier using covariates common to both primary age groups. We hope that such
a classifier will not only identify common predictors of TBI across all ages, but also
simplify the decision-making process by obviating the need for a second classifier.
Nonetheless, in developing new classifiers, priority will be placed on interpretability, 
brevity and simplicity, and a low false negative rate. 

# Data

## Data Collection

Before going into extensive detail about the dataset used in this analysis, as
well as our cleaning and preprocessing steps, we first offer our own thoughts
on what data would be most relevant for vetting previous classifiers and developing
an age-invariant classifier. 

Besides needing a dataset similar in nature
to the dataset on which the original classifiers were based (comprised of children
presenting to ER with head trauma, etc.), to vet the previous
classifiers, we need a dataset that will enable us to develop classifiers with
high predictive accuracy and maximal generalizability so as to compare with previous classifiers. 
For instance, we would need a dataset containing covariates that are both relevant to the presence
of TBIs and can be measured at the time a child is ushered into the ER. From common
sense, as well as consulting with Dr. Robert Inglis of UCSF, it seems important 
information might be the injury mechanism (and a rating of
severity), any symptoms such as headache or amnesia, and any results of a physical
examination that may indicate neurological impact or skull injury. For both
vetting the clinical decision rule and developing an age-invariant rule, it would
be preferable to have covariates that are as objective as possible so as to avoid
issues with interrater reliability and improve generalizability of the rules to 
new, real-world settings. It would also be preferable to both have details on covariates 
invariant to gender, race, ethnicity, or any other such characteristic
of the population of interest and a population with gender, age, and racial diversity,
to reduce the potential of systematic misdiagnoses in these groups
should the rule be applied in future settings. Finally, we wish to identify children
who do not need CT scans among a population where there is already some doubt
as to whether the CT scan will be worth the risk of radiation induced malignancy.
So, the ideal dataset will consist of children who are not critically injured
and who could conceivably be TBI-free.

To develop an age-invariant classifier, the above points are certainly relevant.
However, specific to an age-invariant classifier, we would also need covariates
whose presence or values are not age-dependent.

The dataset used in this analysis is the same one used by Kuppermann and his colleagues
to develop their classification rules, consisting of 43399 children who presented
to the emergency room for head trauma at one of the PECARN network hospitals
between 2004 and 2006. For each
child, the data were collected by physicians who filled out standardized sheets detailing the
child's condition, including both questions for the child ('Do you have a headache?')
and results of a physician examination ('Palpable skull fracture?'). Additionally,
the dataset contains information that would be available after the decision to
pursue a CT scan has been made, such as the reasons why a CT scan is being pursued,
whether a child will need to be sedated for a CT scan, and whether there were 
any traumatic findings on the scan. The outcome variable (TBI) was obtained by
tracking the child's progress in the hospital after the initial examination 
(whether they were admitted for neurological surgery, or died in the ED, for example)
or, if the child left the hospital, following-up with the child's parents, the medical record,
emergency department process improvement records, and county morgue records to ensure
no TBI diagnoses were missed. In total, there are 125 covariates, including the outcome.

It seems that in this dataset, most of the variables we conjecture are important
for predicting TBIs are present, but that there are many covariates whose information
would not be known at the time our classifier would be implemented. Additionally,
many covariates seem more objective than others ('Palpable skull fracture?' vs. 
'Is the child acting normally?'). There are also many variables which are not invariant
to age. For instance, the ability to say you have a headache is reserved for verbal
children who must be above a certain age. These variables and decisions on what to do
with them will be discussed at length in Cleaning and Preprocessing, as well as
Post-Processing EDA. The dataset also provides demographic characteristics such as
age, gender, race, and ethnicity. This information will be discussed at length in 
Post-Processing EDA.

## Cleaning and Preprocessing

In order to achieve a suitable dataset for the predicting low risk of TBI
in children according to the above parameters, we first implement a set of
cleaning and preprocessing steps. The overarching goal of our cleaning
and preprocessing steps is to keep only covariates relevant to predicting TBI
at the time a child would present to the ER, keep only observations for whom
this classification rule would be relevant (children for whom it's a toss-up
as to whether they'll need a CT scan), and to deal with missing values. We present
our procedure below, along with relevant judgement calls and their perturbations
where applicable.

* Remove columns irrelevant to predicting TBI at the time a child enters the emergency room
  + *EmplType*, *Certification*, *Ind...*, *CT...*, *Finding1*, ... *Finding23*, *EDDisposition*, *Observed*, *AgeTwoPlus*, *AgeInMonth*
  + Judgement Call [`injMech`]: drop *InjuryMech* 
    + Perturbation: keep *InjuryMech* 

This includes *EmplType* and *Certification*, any variable about the reasons why a CT scan was ordered, whether sedation would be needed when performing a CT scan, the traumatic findings from the CT scan, the reason why the child left the ER, or 
whether the child was observed again to see if they needed a CT scan. 
We remove the former two variables because domain knowledge
says that we should not expect the qualifications of the physician to systematically
affect the responses collected. The rest of the variables would either not be available
at the time a child first enters the ER, or would be known only after
the decision to obtain a CT scan has been made. So, these are not of interest.
We also remove an indicator for the individuals being above age 2, as well as
the age provided in months, opting to keep the age provided in years. Similarly,
according to judgement call [`injMech`], we drop the injury mechanism, a 13-level categorical variable detailing how the
injury occurred, opting to keep a severity variable, a 3-tiered categorical variable
rating the severity of the injury mechanism as 'low', 'moderate', or 'high'.

* Remove columns with high levels of missingness
  + *Dizzy*, *Ethnicity*

*Dizzy*, an indicator of whether the individual felt dizzy, and *Ethnicity*,
whether the individual was Hispanic, are both missing in approximately 35% of the
observations. *Ethnicity* would be useful for posthoc analysis, but with such a large
number of missing values, it may not be particularly informative. *Dizzy* could be
a good indicator of TBI, but was deemed too subjective and prone to inter-rater reliability
issues according to both Kupperman et al. and Dr. Inglis.

* Remove observations whose *GCS* scores are less than 14

GCS, or Glasgow Coma Score, measures an individual's level of consciousness on a scale from 3 to 15,
and is the sum total of three subscores: an eye score, a verbal score, and a motor score.
If GCS is less than 14, the individual is quite injured and would likely need a CT scan anyway.
We only keep individuals where the total GCS is 14 or 15.

* Impute and Drop based on GCS scores
  + Judgement Call [`missSubGCS`]: drop observations with total GCS of 14, but missing any of the GCS subcategory scores (*GCSVerbal*, *GCSMotor*, *GCSEye*) 
    + Perturbation: keep the observations
  + Impute a full score into each GCS subcategory when total GCS is 15 
  + Judgement Call [`fake15GCS`]: drop observations with total GCS of 15, but suboptimal score in a subcategory 
    + Perturbation: keep the observations
  + Judgement Call [`fake14GCS`]: drop observations with total GCS of 14, but optimal score in every subcategory 
    + Perturbation: keep the observations
  
GCS score is likely an important predictor of TBI, so it is imperative this variable is as completely 
filled-out as possible in this dataset. However, it should be noted that
there are different standards for obtaining GCS scores for individuals younger than two
and those two or older, since individuals younger than two cannot verbalize. We note this
as a potential problem to developing an age-invariant classifier.

* Remove observations who were pharmacologically paralyzed, sedated, or intubated at the time of evaluation, or for whom any of this information is missing
  + *Paralyzed*, *Sedated*, *Intubated*
  
According to domain knowledge, individuals who must be pharmacologically paralyzed, 
sedated, or intubated at the time of evaluation are likely quite injured and do not 
represent the population of interest for these classification rules.

* Drop and impute based on variables of altered mental status
  + *AMS*, *AMSAgitated*, *AMSSleep*, *AMSSlow*, *AMSRepeat*, *AMSOth*
  + Judgement Call [`AMS`]: drop observations missing any of the above variables
    + Perturbation: impute missing *AMS* based on presence of any of the subvariables, and drop the subvariables and any remaining missing *AMS* observations
    
*AMS* is an indicator representing if the child has altered mental status, and
the remaining variables are subvariables indicating the reason why the child has
altered mental status (agitated, sleepy, slow to respond, asking repetitive questions, etc.)
This is likely an important predictor, so it is important to handle missing values.
By judgement call `AMS`, we drop any observation with a missing value in any of these
categories, since if the observation is missing it is impossible to truly know a value
to impute for *AMS*. As a perturbation, we could impute a 1 in *AMS* if any
if any of the subcategories is affirmative, and 0 otherwise. However, it would
be impossible to impute for missing values in the subcategories, so we drop
these observations.

* Drop and impute based on variables of other non-head substantial injuries
  + *OSI*, *OSIExtremity*, *OSICut*, *OSICspine*, *OSIFlank*, *OSIAbdomen*, *OSIPelvis*, *OSIOth*
  + Judgement Call [`OSI`]: same procedure as above for *AMS* and its subvariables
    + Perturbation: same procedure as above for *AMS* and its subvariables
    
*OSI* is an indicator on whether the physician determined the child sustained 
substantial non-head injuries. This could be an injury to the extremities like a fracture,
a cut requiring surgery, a spinal cord injury, a flank injury, an intra-abdominal injury,
or a pelvis injury. Although not directly related to head trauma, we find this variable
might be an indicator of the injury severity. If another variable relevant to head trauma
fails to indicate a TBI for a particular individual, including this variable offers
another opportunity to diagnose the TBI, possibly suppressing the false negative rate.
By judgement call `OSI`, we drop any observations missing any of these values or their
subvariables for the same reason as judgement call `AMS`. 

* Drop and impute based on hematoma information
  + *Hema*, *HemaSize*, *HemaLoc*
  + Judgement Call [`HEMA`]: same procedure as above for *AMS* and its subvariables
    + Perturbation 1: same procedure as above for *AMS* and its subvariables
    + Perturbation 2: Only drop missing *HemaLoc* and *Hema*, impute missing *Hema* from *HemaLoc* presence, drop remaining missing *Hema*
    
*Hema* indicates the presence of a scalp hematoma, or swelling due to some head trauma,
*HemaSize* is a categorical variable representing the size (< 1cm, 1-3cm, > 3cm) and
*HemaLoc* is a categorical variable representing the location (frontal, occipital, parietal/temporal).
According to domain knowledge, scalp hematomas are fairly good predictors of TBI.
By judgement call [`HEMA`], we drop all observations missing any of these categories.
By Perturbation 1, we only keep the indicators for the presence of hematoma. By Perturbation 2, we keep the indicator as well as the location, 
since according to domain knowledge, the impact of head trauma to the brain is dependent on the location of the trauma.

* Drop and impute based on skull fracture variables
  + *SFxPalp*, *SFxPalpDepress*
  + Judgement Call [`SFx`]: recode an unclear *SFxPalp* observation as true, drop observations missing either *SFxPalp* or *SFxPalpDepress*
    + Perturbation 1: same procedure as [`AMS`] perturbation
    + Perturbation 2: same procedure as [`AMS`] judgement call
    
*SFxPalp* is an indicator for a palpable skull fracture, which can also be marked as
'unclear' if there is too much swelling to make a determination. *SFxPalpDepress*
is an indicator on whether the fracture feels depressed. Both of these are likely
important predictors via domain knowledge. As a judgement call, we take an unclear
observation as being a palpable skull fracture, since even if the examination was unclear,
there is still substantial head trauma.

* Drop missing values for bulging Anterior Fontanelle
  + *FontBulg*

*FontBulg* is an indicator for whether the child's anterior fontanelle is bulging,
indicating pressure within the brain. This feature would typically only be present in
infants, since for older individuals their anterior fontanelle will have closed as their
skull hardens with age. While possibly a good predictor for TBI in infants, this
variable does not have much predictive value for older children.

* Drop and impute based on basilar skull fracture variables
  + *SFxBas*, *SFxBasHem*, *SFxBasOto*, *SFxBasPer*, *SFxBasRet*, *SFxBasRhi*
  + Judgement Call [`SFxBas`]: same as [`AMS`] judgement call
    + Perturbation: same as [`AMS`] perturbation

*SFxBas* is an indicator if the physician has determined there is basilar skull fracture
present, or a fracture at the base of the skull. The rest of the variables indicate
common indicators of basilar skull fracture (hemotympanum, otorrhea, raccoon eyes, Battle's sign, rhinorrhea).
More specifically, hemotympanum is when blood is found in the middle ear, otorrhea is when spinal fluid drains
from the ear, raccoon eyes is when internal bleeding occurs below the eye, Battle's sign is a bruise at the 
bottom of the skull, and rhinorrhea is when spinal fluid leaks out the nose.
This is known to be quite an important predictor of
TBI, so we wish to make these variables as complete as possible for an ideal dataset.
To do so, we follow the same procedure for *AMS* and its subvariables, regarding missingness.

* Drop and impute based on above-the-clavicle injury variables
  + *Clav*, *ClavNeck*, *ClavFro*, *ClavOcc*, *ClavPar*, *ClavTem*
  + Judgement Call [`Clav`]: same as [`AMS`] judgement call
    + Perturbation: same as [`AMS`] perturbation

*Clav* is an indicator on whether there was a laceration, abrasion, or hematoma
above the clavicles (neck). The subvariables indicate whether there was trauma on
the neck or the frontal, occipital, parietal or temporal regions of the scalp.
In the same way as *OSI*, this variable will be useful to keep around because it
could indicate a TBI if one of the head trauma variables fails to.

* Drop and impute based on neurological deficit variables
  + *NeuroD*, *NeuroDMotor*, *NeuroDSensory*, *NeuroDCranial*, *NeuroDReflex*, *NeuroDOth*
  + Judgement Call [`Neuro`]: same as [`AMS`] judgement call
    + Perturbation: same as [`AMS`] perturbation
    
*NeuroD* is an indicator for whether or not there is some neurological deficit
besides altered mental status, as determined by the physician. This includes
motor, sensory, cranial nerve (pupil reactivity), or reflex issues. Generally,
these categories are quite important for predicting TBI according to domain knowledge.
As with previous groups of variables, we implement a judgement call to drop
any observation missing any of these values.

* Drop and impute based on vomiting variables
  + *Vomit*, *VomitNbr*, *VomitStart*, *VomitLast*
  + Judgement Call [`Vomit`]: same as [`AMS`] perturbation
    + Perturbation: same as [`AMS`] judgement call

*Vomit* is an indicator for whether the individual vomited (as reported by child
or parents), *VomitNbr* is the number of times the child vomited post-injury (1, 2, > 2 times),
*VomitStart* is how soon after the head injury the vomiting started (within 1 hour, 1 - 4 hours, > 4 hours after), and 
*VomitLast* is how soon before the evaluation was the last
episode (< 1 hour, 1 - 4 hours, > 4 hours before evaluation). As a judgement call,
we only keep *Vomit* and impute missing values from the subvariables, since according
to domain knowledge, more details about vomiting is generall unhelpful. As a perturbation,
we keep these covariates.

* Drop and impute based on headache variables
  + *HA_verb*, *HAStart*, *HASeverity*
  + Judgement Call [`HA`]: Drop missing *HASeverity*, impute missing *HA_verb* from *HASeverity*, and drop remaining missing *HA_verb*, ignore *HAStart* column 
    + Perturbation 1: same as [`AMS`] perturbation
    + Perturbation 2: same as [`AMS`] judgement call
    
*HA_verb* is an indiactor on whether the child had a headache, and is marked differently
if the child is too young to verbalize that they had a headache, or unable to speak.
*HAStart* and *HASeverity* are both variables indicating when the headache began
(before injury, within 1 hour after, 1-4 hours after, > 4 hours after) and the severity
(mild, moderate, and severe). As a judgement call, we keep *HA_verb* and *HASeverity*
since these are both more likely to predict TBI than *HAStart*. However, it should be
noted that this is an age-dependent variable as discussed in the prior section, since
being able to verbalize headache symptoms requires the child to be of a certain age.

* Drop and impute based on seizure variables
  + *Seiz*, *SeizLen*, *SeizOccur*
  + Judgement Call [`Seiz`]: Drop missing *SeizLen*, impute missing *Seiz* from *SeizLen*, drop missing *Seiz*, and ignore *SeizOccur* column
  
*Seiz* is an indicator if the child had a seizure, as reported by the parent or child.
*SeizLen* is how long the seizure lasted (< 1 min, 1-5 min, 5-15 min, >15 min) and
*SeizOccur* is when did it occur relative to the injury time (immediately on contact, 
within 30 minutes, or > 30 minutes after injury). Whether a seizure occured is 
important to consider, and the length is also somewhat important. So, we opt to
keep *SeizLen* and *Seiz*, but drop *SeizOccur*, since this is not as important
according to domain knowledge. 

* Drop and impute based on loss of consciousness variables
  + *LOCSeparate*, *LocLen*
  + Judgement Call [`LOC`]: recode 'suspected' as 'yes' in *LOCSeparate*, and drop observations with missing *LOCSeparate* and *LOCLen*
    + Perturbation 1: same as [`AMS`] perturbation
    + Perturbation 2: same as [`AMS`] judgement call
    
*LOCSeparate* is an indicator if the child lost consciousness at all after the injury,
and *LocLen* is how long the child lost consciousness (< 5 sec, 5 sec - 1 min, 1 - 5 min, > 5 min). 
Loss of consciousness is generally a good predictor of head trauma, so we wish
to include this to predict TBI. As a judgement call, we consider a suspected loss of
consciousness as affirmative, since if there is any doubt as to whether the child
lost consciousness, this should be of concern to err on the side of caution.

* Drop missing values for *Amnesia* and *High_impact_InjSev*

*Amensia* indicates whether the child has amnesia for the event, but can only
be answered by verbal children. This is an example of a variable that is not age-invariant,
although it could be a good predictor for verbal children. *High_impact_InjSev*
is the rating of the injury mechanism severity, as mentioned above. 

* Impute missing values for *ActNorm*
  + Judgement Call [`ActNorm`]: recode null values as affirmative
    + Perturbation: recode values as not affirmative

*ActNorm* is a subjective assessment by the parent as to whether the parent
thinks the child is acting normally. Although subjective, domain experts conclude
that a parents assessement of their child's behavior is a good indicator of
how severely the child may be injured. Because this is important, we wish to impute
as many values as possible. As a judgement call, we consider a missing value
as being normal, since if the child was acting normally, it's possible this
question could have simply been skipped over. As a perturbation, we reverse this
judgement call.

* Make a pooled outcome variable

We define the outcome, a clinically-important TBI, in the same way as Kuppermann
and his colleagues, except we impute missing values in the outcome by the
presence of any of the following: death due to TBI, intubated for more than 24 hours,
neurosurgery was performed, and the individual was hospitalized for more than 2 nights
due to head injury. This should give a complete assessment as to whether an individual
had a TBI.

## Data Summary

Once we have cleaned the data according to the above steps, we wish to extract
binary features from the categorical variables, where applicable. For every subvariable/
parent variable pairing mentioned above (for example, `NeuroDMotor` and `NeuroD`),
if the parent variable is marked as false, the subvariable has a value indicating
it is missing. As a default, we maintain this missing category, and binarize the variables
by one-hot encoding. Alternatively, we could impute false into the missing values
and drop the parent variables, so now each subvariable indicates the presence of the
subvariable AND the parent variable as opposed to the subvariable GIVEN the parent variable.

After performing this one-hot encoding, we have 137 features in total. We split 
the data into a training set (19947 observations), a validation set (6649 observations), 
and a test set (6649 observations). For post-processing EDA, we only consider
the training set, only using the test set to check model accuracy. We should note
that there may be cluster structure in the dataset by virtue of the data
being collected from different hospitals, presenting a possible comparability issue.
However, we are not provided with hospital information,
so we perform a random train-validation-test split.

## Exploratory Data Analysis

Data Collection detailed a list of characteristics of an ideal dataset to vet and
improve upon the original decision rule, according to the goals outlined in Introduction
and Problem Motivation.
In this section, we explore the relevance of the cleaned and preprocessed data to answer
the question at hand, assessing how the data meets these ideals via exploratory plots. 
The results of our exploratory analysis
will then be used to motivate decisions regarding feature inclusion and sample splitting
as we build our classifiers.

Of course, the dataset already meets some of these conditions by virtue of our cleaning
process: the cleaned dataset only includes individuals with GCS scores of 14 or 15,
and individuals who were not so injured as to be intubated, pharmacologically paralyzed,
or sedated at the time of evaluation. Additionally, since we are using the same original
dataset as Kuppermann et al., where children presented to the ER with head trauma,
our results can be suitably compared to theirs. Furthermore, since our data was cleaned
differently from theirs, we get an added stability assessment of their decision rule for
free, a crucial element of vetting their work.

As mentioned in Data Collection, one of the primary criteria of a good dataset
for this problem is a dataset which is diverse in demographic characteristics,
including age, race, and gender. This is crucial for generalizability of our work,
as a classifier built from a dataset that is demographically lopsided could exhibit
unforeseen biases when exposed to more diverse data.

```{r demographics, echo = FALSE, fig.cap = 'Distributions of various demographic variables in the training dataset, after cleaning and preprocessing.', out.width = '100%'}
include_graphics('figs/demographics.png')
```


Figure \ref{fig:demographics} shows distributions of various demographic variables
in the cleaned training dataset. In the top-left plot, we see that younger ages
are generally represented more than older ages. However, younger children (2 or younger)
share many characteristics, including that they are likely preverbal, whereas older
children (older than 2) are likely verbal. With this split, it seems like there would
be sufficient representation for each age group. Regardless, there are at least 1500
observations for each age out of 19947 total observations, which we find to be sufficient
representation.

In the top-right plot, we see that there are about 8000 females and 12000 males in the dataset.
While we wouldn't expect different indicators for traumatic brain injury by gender,
the dataset is a bit lopsided in this regard; perhaps young boys tend to engage 
in more dangerous activities than young girls, and thus are at greater risk of head injury. 
Similarly, in the bottom-right plot we see that the vast majority of observations are White or Black, while
other races are quite underrepresented by comparison. Once again, we would not expect
a race-dependence of the indicators for TBIs, but like with the observed gender imbalance, 
we will explore the potential impact of this racial imbalance in Post-Hoc Analysis.

In the bottom left plot, we see that almost 20% of the TBIs observed are in children
aged 0, and that the number of TBIs decreases as age increases, flatlining on average
for children older than 2. Similarly with the age imbalance, 
if we were to split by young children versus older children, the number of TBIs in each age group
would be more balanced and likely sufficient to generate a classification rule.

While an imbalance in outcome across ages may skew our
age-invariant classifier to detect rules relevant for one age group versus another,
we should be careful to only include predictors whose relationships with the outcome
are invariant by age. The next few plots provide evidence of age dependence
and lack thereof for certain variables, motivating the use of certain features
for an age-invariant classifier. 

First, Figure \ref{fig:age_preverb} shows
the distribution of 'verbal' vs. 'non-verbal' or 'pre-verbal' in response
to the physician asking the child if they have amnesia or any headache symptoms.
We clearly see that, among children aged two or younger, more than 80% of them
were marked as non-verbal in response to both questions. So, a rule based on
whether or not the individual has a headache or amnesia would be useless for these
individuals. However, among children older than aged two, more than 90% of 
individuals were able to verbalize answers to these questions. These variables
would likely be good predictors of a TBI, so for a classifier only on older individuals,
these variables would be useful to include. However, they should not be included
in any classifier for younger children, or for an age-invariant classifier.

```{r age_preverb, echo = FALSE, fig.cap = 'Distributions of \'Verbal\' or \'Non-Verbal/Pre-Verbal\' responses to questions on amnesia and headache symptoms, by age.', out.width = "100%"}
include_graphics('figs/age_preverb.png')
```

Next, we recall that, according to Kuppermann et al., for children two or younger,
a Pediatric GCS score was assigned, whereas for children older than two, a standard
GCS score was assigned. In essence, these are two different standards for evaluating
the consciousness of children, since children of different ages have different
abilities in expressing their consciousness. However, because the standards are
different across these two age groups, we fear that a suboptimal score with regard
to one standard might be more indicative of a TBI than a suboptimal score with respect
to the other standard. If GCS scores are included in an age-invariant classifier, 
this presents a potential comparability issue, where the rule that is learned may be
applicable to one age-group but not the other. Figure \ref{fig:gcs_age} shows the
distribution of the outcome variable for suboptimal versus optimal scores of each
GCS category (row) for the different age groups (column).

```{r gcs_age, fig.cap = 'Distributions of outcome among different GCS scores (row) for different age groups (column). 1 indicates optimal score in a GCS category, 0 is one point suboptimal (to achieve total GCS of 14 instead of 15).', out.width = '100%', echo = FALSE}
include_graphics('figs/gsc_age_dep.png')
```

In this figure, we see that, for the GCS Eye scores and the GCS Verbal scores, the outcome conditional on suboptimal versus
optimal scores is distributed similarly across both age groups. This indicates that
the relationship between GCS score in these categories and the outcome is age invariant.
However, for the GCS Motor scores, we see that the distributions
are not the same. In fact, it seems like for individuals older than age 2, half
of individuals that had a suboptimal GCS Motor score ended up having a TBI, whereas
among children two or younger, a much smaller proportion of those with a suboptimal GCS Motor
score had a TBI. This tells us that there is likely some age dependence in how this
variable is related to TBI. So, when building an age-invariant classifier we also
wish to exclude the GCS scores.

Before moving on, we should also establish the age-invariant relationships of other
variables of interest with the outcome variable. Figure \ref{fig:no_age_dep}
shows that the distributions of the outcome conditional on having neurological deficits
or not, and having basilar skull fracture or not, are not age-dependent. This is as we would 
expect, since these injuries are observed directly by doctors and are not dependent upon 
verbal ability or any other factor directly associated with age.
Since the relationship of these indicators with the outcome are invariant to age,
we should certainly include variables like these in an age-invariant classifier.

```{r no_age_dep, fig.cap = 'Distributions of outcome conditional on neurological deficit and basilar skull fracture, across both age groups (1 indicates the presence of neurological deficit/basilar skull fracture).', out.width = "100%", echo = FALSE}
include_graphics('figs/no_age_dep.png')
```

Finally, in making a classifier to vet and possibly improve upon that of Kuppermann et al.,
we explore the potential impact of including and excluding subvariable details. Recall
from Cleaning and Preprocessing that there are many categories which can be thought
of as parent categories (Altered mental status? Neurological Deficit? Basilar skull fracture?),
with children subcategories that provide more detailed information about the parent
category (if altered mental status, is it because you are sleepy? Agitated? Asking repetitive questions?).
In making an interpretable classifier with high predictive accuracy and a low false
negative rate with which to compare to Kuppermann's, there are arguments for and 
against including this subvariable information.

On the one hand, it is possible that certain subvariables are more predictive
of the outcome than other subvariables. Consider Figure \ref{fig:subvariable_arg},
which shows the correlations of the different subvariables for two parent variables, altered mental status
and basilar skull fracture, with the outcome. Each subvariable is binary encoded:
if the individual has any of the qualities, it is marked as 1, otherwise, it is marked
as 0. For AMS, it seems 'Sleepy', 'Slow to Respond', and 'Agitated' are much more correlated
with the outcome than 'Repetitive'. Even more so, for basilar skull fracture, it seems
'Hemotympanum' is much more highly correlated with the outcome than the other
variables. If we were to exclude this subvariable information in lieu of only including
an indicator for basilar skull fracture, it is possible that we could over-indicate
TBIs. Imagine if a child came into the emergency room with CSF Rhinorrhea, 
but our classifier only considers whether there is a sign of skull fracture.
Skull fracture, as an umbrella variable, might be an important predictor, but only because hemotympanum is so correlated
with the outcome. Thus, the child would probably be recommended to have a CT scan. However,
the child may not have had a TBI in the first place because CSF Rhinorrhea is not
very correlated with the outcome. Since we would be giving a child an unneeded CT scan,
it could be important to include this subvariable information in an ideal dataset.

```{r subvariable_arg, fig.cap='Subvariables of AMS (altered mental status) and basilar skull fracture are correlated differently with the outcome.', out.width='100%', echo = FALSE}
include_graphics('figs/subvariable_arg.png')
```

However, a clear argument against including this subvariable information is that
including too many variables may lead to very uninterpretable and nonsensical rules
that are the result of overfitting. Additionally, it seems like the parent variables alone
are substantially correlated with the outcome, as is shown in Figures \ref{fig:younger_corr}
and \ref{fig:older_corr}. So, in order to achieve a suitable dataset to make
interpretable and accurate classifiers with which to vet and improve upon Kuppermann's, it seems like
it may be suitable to just include the parent variables. Note also that the following two
plots further show the dataset is suitable for making an age-invariant classifier:
among the age-invariant variables, the correlations with the outcome are almost
identical across age groups.

```{r younger_corr, fig.cap = 'Among children 2 and younger, relevant parent variables seem substantially correlated with the outcome.', out.width='100%', echo = FALSE}
include_graphics('figs/parent_younger.png')
```

```{r older_corr, fig.cap = 'Among children older than 2, relevant parent variables seem substantially correlated with the outcome.', out.width='100%', echo = FALSE}
include_graphics('figs/parent_older.png')
```

# Models

## Baseline Model

We choose the model proposed by Kuppermann and his colleagues as the baseline model
with which to predict whether a child will have a clinically-important TBI.
To evaluate their model, we also use the provided dataset cleaned similarly to theirs, 
consisting of 42412 children presenting to the emergency room 
due to the head trauma with GCS scores 14 and above and with the final outcome non-missing.
376 of these observations experienced clinically-important TBIs.
In addition, we also use the same age splitting method as Kuppermann,
where we use their baseline classifier for preverbal children (< 2 years of age)
to predict TBI in this subgroup, and the classifier for verbal children (2 years and older)
to predict TBI in this subgroup.

### Description

The age-based classifiers in Kuppermann et al. are rule-list models, or a models consisting of a list of rules, 
ordered by importance in indicating a TBI.
A child is labelled as having a TBI if and only if this 
child satisfies at least one of the rules, where the rules are checked in the aforementioned order of importance.
Once we come across a rule that the child satisfies, we stop checking. 
Therefore, based on the order, each rule is applied to different numbers of children.
The list below gives the rules for Kuppermann's pre-verbal (younger than 2) classifier.
Refer to Data Collection for a more in-depth description of the variables mentioned in each rule.

* Altered mental status (*AMS*)? If yes, TBI. If not, continue:

* Occipital, Parietal, or Temporal scalp hematoma (*HemaLoc*)? If yes, TBI. If not, continue:

* Loss of consciousness for more than 5 seconds (*LOCLen*)? If yes, TBI. If not, continue:

* Injury mechanism rated severe (*High_impact_InjSev*)? If yes, TBI. If not, continue:

* Palpable or unclear skull fracture (*SFxPalp*)? If yes, TBI. If not, continue:

* Acting normally according to parent (*ActNorm*)? If yes, TBI. If not, no TBI.

The list below now gives the rules for Kuppermann's verbal (2 or older) classifier.

* Altered mental status (*AMS*)? If yes, TBI. If not, continue:

* Loss of consciousness for more than 5 seconds (*LOCLen*)? If yes, TBI. If not, continue:

* Injury mechanism rated severe (*High_impact_InjSev*)? If yes, TBI. If not, continue:

* Basilar skull fracture (*SFxBas*)? If yes, TBI. If not, continue:

* Severe headache (*HA_Severity*)? If yes, TBI. If not, no TBI.

### Procedure and Performance

```{r baseline_res, echo = FALSE, fig.cap = 'Decision rule lists for clinically-important traumatic brain injury in children younger than 2 years and in those aged 2 years and older.'}
include_graphics('figs/baseline_res.png')
```

The result of baseline model is shown in \ref{fig:baseline_res}. The yellow blocks are identified as positive 
while the green blocks are identified as negative. In each block, the first number represents the number of children
experiencing clinically-important TBI, the second number is the number of children in this block, and
the last one represents the proportion. And we use the whole cleaned dataset to evaluate the model, 
while Kuppermann et al. splits the data into training set and validation set. The performance of the baseline classifiers
is similar to the result in Kuppermann et al. The baseline classifiers do identify almost all of the cases since 
the sensitivity is 98.9\% and 96.8\% for preverbal children and verbal children, respectively. However, the specificity only achieves
48.3\% and 57.7\%. In our following model, we will propose the one with similar sensitivity and higher specificity. The detailed
evaluation metrics based on sensitivity and specificity is discussed in the following part.

## Proposed Model

*Describe the proposed model, it's advantages, disadvantages, how it might compare to the original model, etc.*

*Justify the proposed model, and refer back to discussion in Post-Processing Discussion for justification while also answering Q8 about any randomness assumptions. *


### Introduction to and an ilustration of `rulefit`

*(Andrej)*

```{python basic_rules, engine.path = "../../../rule-env/bin/python3", include=FALSE}
import pickle as pk
import imodels


basic = pk.load(open("notebooks/models/basic.pkl", 'rb'))
maximal = pk.load(open("notebooks/models/basic.pkl", 'rb'))

brules = basic.get_rules().sort_values("importance", ascending=False)
mrules = maximal.get_rules().sort_values("importance", ascending=False)

```

`#TODO: round the numbers, print less rules`

```{r, echo=FALSE}
kable(py$brules, caption = "Basic model rules in order of importance")

```

```{r, echo=FALSE}
kable(py$mrules, caption = "Maximal model rules in order of importance")
```

To see how these relatively basic models perform, we examine their *sensitivity - specificity* curve:

```{r basic_ROC, echo = FALSE, fig.width=6, fig.height=4, fig.cap="Sensitivity-specificity curve for two basic rulefit models with maximal and minimal judgement calls."}
include_graphics('figs/basic_ROC.png', dpi=500)
```

The minimal model is able to achieve 80%+ in both sensitivity and specificity for one
threshold value; since we care much more about false negatives than positives, we consequently value
sensitivity far above specificity. Therefore in this case, the fuller model with sensitivities
above 90% would be preferred.



///////////////////////////////////////////////////////////////////////////////















(Xin) I think Andrej will describe rulefit model. Q8 I can only answer this in Logistic regression assumption. Can this be better?

The greedy rule list (CART) model consists of a list of rules, which is similar to the baseline model. 
Therefore, it is simple to understand, interpret, and pick the important features.
Since it is also a greedy method, it does not require distributional assumptions and is computationally-efficient.
However, it is non-robust when we perturb the data and the greedy method does not gurantee a good result.

### Procedure and Performance

We train and evaluate the models on our cleaned data based on different age-splitting methods and different judgement calls.
There are three possible splitting methods in age groups: preverbal children, verbal children, and all children.
And we include two different judgement calls: whether include or exclude all subvariables.
Although there are six possible datasets, the dataset of preverbal children excluding subvariables contain too few
TBI children so we cannot get a reasonable result. Therefore, we will train and evaluate for other five datasets.

For each possible dataset, we split 20\% data into the test data, which will only be used in the model evaluation,
while we use 80\% remained data to fine-tune the hyperparameters and fit the model.
Then we split four cross-validation folds to tune the hyperparameters for each model.
Since this is an unbalanced dataset, the classification accuracy cannot be a good measure to evaluate the model.
The best classifier should identify almost all of TBI cases, as well as identify most of children not experiencing TBI as no TBI. 
Therefore, we define our evaluation measure as follows.

\[ \text{score}=\left \{
\begin{aligned} 
& 5 \times \text{sensitivity} + \text{specificity}, \quad \text{if } \text{specificity} > 0.5.\\
&0, \quad \text{if } \text{specificity} \le 0.5.
\end{aligned}
\right.
\]

For greedy rule list model, the hyperparameters are class weights (representing the importance of children experiencing TBI) and 
the maximum number of rules in the list (representing the complexity and the interpretability); for rulefit model, 
the hyperparameter is the maximum length for each rule. In addition, since rulefit model involves random forest methods to pick the rules, 
the performance may vary even if we choose same hyperparameters. Therefore, for rulefit model, we also need to run the models 
with different random states and pick the best one as the result, given the maximum length for each rule. Based on the cross-validation results, we pick the best hyperparameters and evaluate the performance on our test data.

The greedy rule list model performs quite bad. For two datasets excluding the subvariables, the largest mean scores 
in our CV folds are even smaller than 3, which implies the sensitivity is smaller than 60\%. For the datasets including subvariables,
although the largest mean scores are over 4, the scores on the test data is much smaller. This means the greedy rule list model
is very sensitive to the training data and will not behave well on the test data.

```{r rule_result_on_test, echo = FALSE, fig.cap = 'ROC curves of fine-tuned rulefit models on the test data for all five datasets.', out.width = "60%"}
include_graphics('figs/rule_result_on_test.png')
```

The performance of rulefit model on test data is in Figure \ref{fig:rule_result_on_test}. We can find that the performance for age-invariant group is quite well. As the sensitivity achieves almost 100\%, the specificity is larger than 70\%. And including subvariables
increases the performance of classifying the verbal children. Among two verbal children datasets, when sensitivity is larger than 95\%,
the specificity is about 70\% when we include subvariables, while it drops to 55\% if we exclude them. Finally, for preverbal children,
the specificity is around 60\% when the sensitivity is about 95\%. These results imply us developing the age-invariant classifier.

```{r rulefit_interpretation, echo = FALSE, fig.cap = 'The important rules for the age-invariant classifier', out.width = "100%"}
include_graphics('figs/rulefit_interpretation.png')
```

In Figure \ref{fig:rule_result_on_test}, we lists the important rules for the age-invariant classifier.
We find that other signs of altered mental status is the most important linear factor. Besides it, loss of consciousness length 
and history of vomiting are also two important linear factors. The other important rules consists of several variables.
History of vomiting and basilar skull fracture(hemotympanum) forms an important rule.
And another important rule involves the severity of injury, loss of consciousness length, 
feeling depressed in palpable skull fracture, and Neurological deficit(cranial nerve).


### Stability Analysis

Answer Question 9

Go through the perturbations of our judgement calls (which will have been introduced above)
and compare the performance of our classifier, see if it is similar or different

Also, if there was instability in training the classifiers (as Xin noted in his notebook
in our 12/2 meeting), run through it here as well

# Discussion

Discuss the performance, the results, the stability subject to perturbations,
how well it compared to the baseline, etc.

Discuss future directions on what we can do (other models, other perturbations,
things we missed that we would have done with more time, etc.)

# Conclusion

Just wrap it up

