---
title: \textbf{ Predicting Cervical Spine Injuries in Children}
subtitle: "Final Project: Statistics 215A, Fall 2021"
author: "Andy Shen, Licong Lin, Seunghoon Paik"
header-includes:
   - \usepackage{float}
   - \usepackage{bbm}
   - \usepackage{graphicx}
   - \usepackage{array}
   - \usepackage{blindtext}
   - \usepackage{color}
   - \usepackage{subfloat}
output: 
  pdf_document:
    number_sections: true
    toc: true
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
rm(list = ls())
library(tidyverse)
library(knitr)
library(reshape2)
library(patchwork)
knitr::opts_chunk$set(
  echo = FALSE,  # don't print the code chunk
  warning = FALSE,  # don't print warnings
  message = FALSE,  # don't print messages
  fig.width = 6,  # set default width of figures
  fig.height = 4,  # set default height of figures
  fig.align = "center",  # always align figure in center
  fig.pos = "H",  # always plot figure at the exact location of the code chunk
  cache = FALSE,
  dpi = 400) 
```

\newcommand{\blue}{\textcolor{blue}}
\newcommand{\red}{\textcolor{red}}
\newcommand{\bb}{\textbf}


```{r}
# load processed data
train <- read.csv("../../../../data/csi_pecarn/processed/train.csv")
tune <- read.csv("../../../../data/csi_pecarn/processed/train.csv")
test <- read.csv("../../../../data/csi_pecarn/processed/train.csv")
all <- rbind(train, tune, test)[, -1]
```


\newpage

# Introduction and Domain Problem {#sec:intro}

Cervical spine injuries (CSI) include injuries sustained to the spinal cord connecting the head and neck to the rest of the body. These injuries, albeit rare, are extremely severe for children whose bodies are still developing. CSI detection strategies include the use of a **computed tomography (CT) scan**, among others. While these interventions are generally effective at pinpointing injuries, they have the potential to inadvertently expose children to ionizing radiation and cause unnecessary pain (@leonard2011factors). 

The rarity of a CSI, in conjunction with the potentially harmful effects of CSI detection methods, force clinicians and specialists to assess the risk of performing a CT scan against the potential of missing critical injury diagnoses. Predictive capabilities in this regime are supported through a **clinical decision rule (CDR)** which uses prior data of potential CSI injuries to predict whether a patient has sustained a true CSI or not. Doctors use the results of the CDR and their own assessment of the situation to decide if the risk of a CT scan outweighs its potential benefits. 
\red{Include more about how to IMPROVE the current CDR, most likely after we finish our model.}

In this project, we develop a CDR for predicting CSI in children, based on data provided by the Pediatric Emergency Care Applied Research Network (PECARN). The data are described in Section \blue{\ref{sec:data}}. **Our objective is to predict the true outcome of a patient's trauma (whether they truly have a CSI).** We ensure all components of our CDR are interpretable and realistically accessible to physicians when speculation of a CSI is called into question. Throughout the modeling process, we ensure that our predictions are accurate, stable, and capture the breadth of possible injuries and the varying medical and demographic history of each patient.   

The structure of this report is as follows: In Section \blue{\ref{sec:data}}, we explain our data and the features used to predict CSI. In...


# Data Collection {#sec:data}


\red{The questions below are useful to ask: How were the data collected? At what locations? Over what time period? Who collected them? What instruments were used? Have the operators and instruments changed over the period? Try to imagine yourself at the data collection site physically.}

\red{INCLUDE NUMBER OF CONTROL AND CASE PATIENTS IN THE FINAL CLEANED DATA (52).}

As mentioned above, our data is collected from the PECARN hospital network, which contains patient CSI data from a previous CDR study. The previous study contained children both with and without CSI. The children without CSI were grouped into various controls based on their injury mechanism and whether they had received out-of-patient emergency medical services (EMS) treatment. \blue{In our study, we are only interested in distinguishing true injury from simple trauma. We group all of the sub-control groups into a single control group for more refined predictions.}

The data consist of 12 datasets, each containing various categories of information about the patients, such as demographics, medical history, mechanism of injury, their appearance at the time of response and arrival at the hospital, among others. 

Some of the datasets have the same name, but are denoted with an `field`, `out` or `site` at the end of the file name (e.g `clinicalpresentationfield` vs `clinicalpresentationsite`). The suffix "field" denotes measurements of the patient at the location where the injury was sustained, such as the site of the car crash or the patient's home. The suffix "out" refers to information reported by an outside hospital referring the patient to the PECARN system. Note that this information is not available for patients who were not referred by an outside hospital. The suffix "site" refers to information collected at the PECARN hospital site. **The site information is always collected last**, with the outside hospital information collected between field and site if pertinent. 

It is also important to recognize that this data is extremely prone to human and/or machine error. We describe potential lapses in the data collection process in Section \blue{\ref{sec:meaning}}. The data cleaning process is discussed in Section \blue{\ref{sec:clean}}.



## Meaning {#sec:meaning}

In order to better group the variables in our data, we separate them into **three distinct categories: demographic information, injury mechanism, and trauma presentation.** Demographic information simply refers to the patient's basic information, such as age, gender, and ethnicity. While these features may have very little predictive power, they can be useful in grouping injured or non-injured patients post-hoc.

Injury mechanism refers to how the patient sustained their trauma. Examples of injury mechanism include a vehicle accident, child abuse, assault, or falling. Each category of injury is then further elucidated into more specific modes of injury (such as a rear-end collision vs a side impact collision). Most mechanisms of injury in the given data are injuries sustained from various youth sports or from a motor vehicle accident. \red{Discuss later whether mechanism is actually useful or not}.

Finally, trauma presentation refers to the condition of the patient when they arrive to the hospital or at the site of trauma. These features are spanned across multiple datasets. Features in this category include whether the patient is conscious or not, whether they report neck or facial pain, whether their mental status appears to be normal, among others. 

Note here that some features across these three categories preclude accurate reporting of others. For example, if a patient is unconscious or cannot communicate properly, they will be unable to properly describe whether they are in pain. 

For our study, we prioritize using the "site" data if it is available, imputing necessary missing values from outside hospital data and site data in that order. We select the site data since it is most recent in terms of a patient's trauma presentation. Features in the other two categories do not differ across location and are named in an unsuffixed dataset.

The non-demographic variables all measure some aspect of the patient's trauma and allows us to assess how serious it is.  Each piece of data was recorded from multiple different perspectives. The measurements were collected differently across 2-3 different locations and at 2-3 different time periods - the actual values could differ across time and location. Moreover, the instruments used to take each measurement could be calibrated differently or the individual taking the measurement could have made a mistake. It is very important to keep these sources of error in mind in the modeling process and when questioning unusual observations.

By assuming our data is correct, we are assuming that...

\red{What does each variable mean in the data? What does it measure? Does it measure what it is supposed to measure? How could things go wrong? What statistical assumptions is one making by assuming things didnâ€™t go wrong? (Knowing the data collection process helps here.)}

\red{Meaning of each variable -- ask students to imagine being there at the ER and giving a Glasgow coma score, for example, and also a couple of variables -- ask students what could cause different values written down.}

\red{How were the data cleaned? By whom?}

## Relevance

\red{Can the data collected answer the substantive question(s) in whole or in part? If not, what other data should one collect? The points made in (2) are pertinent here.}

The data provided are relevant to our study. The features described in our model are all accessible to a physician when they make their clinical decision. If any additional data is necessary, it could include other potentially useful features that are typically made available to a physician at the time of making the clinical decision. We do not collect any more data since we are provided with all of the data recorded by PECARN and it is highly unlikely that additional data with predictive power exists.


# Exploratory Data Analysis {#sec:eda}

## Reducing the feature space

\red{\textbf{ZZQ I want to make this a bit better.}}

Our initial feature space included over 300 features and an interpretable model only requires a small fraction of these features. We trim down our initial subset of features through the consultation of domain experts in cervical spine injury. Utilizing the assistance of Dr. Michael Boyle, a physician in the UC San Francisco School of Medicine, we reduced our feature space to roughly 51 variables to perform variable selection on. The feature selection criteria is discussed in Section \blue{\ref{sec:feat-sel}}.

The omitted features were mostly redundant information or information that would not be provided to a physician when making a clinical decision. These omissions were largely determined through domain-expert judgment calls. When in doubt, we retained a predictor to avoid potentially losing crucial predictive information. As discussed in Section \blue{\ref{sec:feat-sel}}, we use a variable selection process to determine our best features for modeling.


## Data Cleaning {#sec:clean}

After reducing the feature space, we implement several layers of ***judgment calls*** in our data cleaning procedure. Any judgment call that directly involved medical subject-matter expertise was verified by Dr. Boyle.

\underline{\textit{\textbf{Missing values}}:} Certain columns in the data contain blank or missing values. Features with a proportion of missing values above 15% are removed from this analysis. The remaining missing values are imputed using *defensive imputation* which assumes the worst-case scenario for the patient. For instance, if we do not have data on whether a patient has tenderness in their neck, we assume they do. Note that this does not default to imputing all missing values with "yes." We must consider what each feature is asking for and determine the worst-case scenario accordingly.


\underline{\textit{\textbf{Categorical features}}:} In order to create an interpretable decision rule, we map our categorical features into binary 0-1 features, **with 1 corresponding to "yes" and 0 corresponding to "no".** To do this, we maintain the defensive imputation strategy by determining the worst-case outcome for each feature. Fortunately, the data contains documentation which provides meaning to the values pertaining to each feature. For most features, this consisted of mapping unknown values using defensive imputation, and mapping other variables using common sense. For example, a value of `S` which stabds for "suspected, but unknown" would map to "yes".

\underline{\textit{\textbf{Ordinal features}}:} Our feature space contains two ordinal features: `AVPU` and `TotalGCS`. AVPU stands for "Alert, Verbal, Pain, Unresponsive", and is a widely accepted health care protocol to determine a patient's overall status. Each letter in "AVPU" corresponds to a more urgent patient status. Alertness and unresponsiveness are simply whether the patient is alert or unresponsive. "Verbal" and "Pain" correspond to whether the patient is receptive to verbal or painful stimulation. In our case, we treat anything that is not alert as the worst-case scenario and impute accordingly. Failing to respond to any stimuli whatsoever should be heavily scrutinized and our imputation takes this into account.

The Glasglow Coma Scale is a similar score used to determine one's consciousness level. It is a sum of three sub-tests and is scored on a scale from 3 (completely unconscious) to 15 (fully conscious). Here, we classify any score above 3 as the worst-case scenario.


## Correlation between features

Figure \blue{\ref{fig:corr-map}} shows a correlation heatmap of the reduced feature space of size `r ncol(all)`. While most features are not strongly correlated with each other, there are some features that exhibit high correlations. These features are typically those of the same type, such as whether the patient reported pain in their neck and whether tenderness was observed in the patient's neck. It is expected for features like these to be influenced by one another. 

Moreover, most features have absolute correlation $|\rho| < 0.5$, indicating weak association throughout the feature space in general. Because of this, we feel confident that we will not encounter any issues surrounding variance inflation and multicollinearity in the modeling stage.

\begin{figure}[H]
\includegraphics[height=6in, width=8in]{corr-map.png}
\centering
\caption{Correlation heatmap of reduced feature space.} 
\label{fig:corr-map}
\end{figure}

## Correlation with outcome {#sec:corr-out}

After seeing which features are associated with each other, we then examine the correlation of each feature with respect to the outcome variable (whether the patient truly has a CSI). We find that most features are not highly correlated with the outcome in either direction, with some exceptions: the `IntervForCervicalStab` feature, which tells whether the patient underwent any cervical stabilization measures at the site, has the highest correlation (above 0.60) with the outcome. This should not be surprising since a cervical stabilization measure is only imposed on a patient if a medical official on-site thinks it is necessary, thus we expect most true injuries to be highly associated with this intervention. 

Similarly, `DxCspineInjury` measures whether the patient is suspected of having a CSI. This has the second-highest correlation of roughly 0.45, which also should not be surprising. We discuss this variable in more detail in Section \blue{\ref{sec:best}}. 

## Frequency of feature values

```{r}
outcomes <- all %>% filter(outcome == 1) %>% 
  select(-outcome) %>% select(!contains("2"))
freq1 <- sort(apply(outcomes, 2, mean), dec = TRUE)
freq0 <- 1 - freq1
outcome_freq <- data.frame(cbind(freq0, freq1)) %>% 
  rownames_to_column("feature") %>% 
  mutate(diff = abs(freq1 - freq0)) %>% 
  arrange(desc(diff))
```


```{r}
controls <- all %>% filter(outcome != 1) %>% 
  select(-outcome) %>% select(!contains("2"))
freq1 <- sort(apply(controls, 2, mean), dec = TRUE)
freq0 <- 1 - freq1
control_freq <- data.frame(cbind(freq0, freq1)) %>% 
  rownames_to_column("feature") %>% 
  mutate(diff = abs(freq1 - freq0)) %>% 
  arrange(desc(diff))
```


We further explore our data by examining the frequency of "yes" and "no" values in for each feature. The six features with the greatest and smallest *absolute differences* in relative frequency are shown in Figures \blue{\ref{fig:largediff}} and \blue{\ref{fig:smalldiff}}, respectively.

```{r}
plotRelFreq <- function(data, high = TRUE) {
  library(tidyverse)
  if (high) {
    d <- head(data, 5)
  } else {
    d <- tail(data, 5)
  }
  d %>% select(-diff) %>% melt() %>% 
    ggplot(mapping = aes(x = reorder(feature, -value, sum), y = value, fill = variable)) + 
    geom_col(position = 'dodge') + 
    labs(x = "Feature", y = "Relative Frequency") +
    scale_fill_manual("Value", values = c("mediumpurple3", "mediumseagreen")) + 
    theme_classic() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}
```

```{r outcomes, fig.cap="Top five features with the greatest absolute differences in relative frequency.", out.width='90%', fig.width=9, fig.height=5}
outcome_high <- plotRelFreq(outcome_freq)
outcome_low <- plotRelFreq(outcome_freq, high = FALSE)
outcome_high + outcome_low
```

```{r controls, fig.cap="Top five features with the greatest absolute differences in relative frequency.", out.width='90%', fig.width=9, fig.height=5}
controls_high <- plotRelFreq(control_freq)
controls_low <- plotRelFreq(control_freq, high = FALSE)
controls_high + controls_low
```




# Modeling {#sec:modeling}

## Translation

\red{How should one translate the question in (1) into a statistical question regarding the data to best answer the original question? Are there multiple translations? For example, can we translate the question into a prediction problem or an inference problem regarding a statistical model? List the pros and cons of each translation relative to answering the substantive question before choosing a model.}

\red{Do we have multiple reasonable translations?}

Our statistical goal is to accurately diagnose a cervical spine injury while keeping the Type II error rate as low as possible. In the context of CSI, a Type II error results when our CDR determines no CSI is present, when this is not the case in reality. This statistical flaw bears serious and life-threatening consequences on children, and we must pay special attention to this rate. 

However, if the false negative rate is our only area of concern, there is no need for any decision rule since a clinician would diagnose a CSI and perform a CT scan for all situations. As mentioned in the Introduction (Section \blue{\ref{sec:intro}}), CT scans could result in unnecessary exposure to ionizing radiation, which is als harmful. Therefore, we must strike a balance between taking the conservative route of always diagnosing someone with CSI and the worst-case scenario of not diagnosing a serious injury. 


## Feature and hyperparameter selection {#sec:feat-sel}

We use a combination of **logistic regression** and **backwards selection** to choose our model features. 

\textcolor{red}{\textbf{ZZQ Licong, you will need to write about this since you did it. Or you can delineate the method for me. Please include the best features selected.}}

## Models

### AdaBoost

\textcolor{red}{\textbf{ZZQ Licong or Hoon, please provide a description for each, as well as diagrams if applicable.}}

### Decision Tree

\textcolor{red}{\textbf{ZZQ Licong or Hoon, please provide a description for each, as well as diagrams if applicable.}}

I think a diagram for the decision tree would be helpful.

### Logistic Regression

\textcolor{red}{\textbf{ZZQ Licong or Hoon, please provide a description for each, as well as diagrams if applicable.}}


## Best model {#sec:best}


```{r accuracy-rates, out.width="85%", fig.width=7.5, fig.cap="Sensitivity and specificity rates for three candidate models: logistic regression, decision tree, and AdaBoost. Accuracy rates are assessed at each model's optimal hyperparameter value."}
type <- c("AdaBoost", "Decision Tree", "Logistic Regression")
vars <- paste0(c(16, 6, 21), " optimal features")
sensitivity <- c(0.97196262, 0.95327103, 0.97196262)
specificity <- c(0.78956835, 0.76438849, 0.79136691)
names(sensitivity) <- names(specificity) <- type
df_res <- data.frame(type, sensitivity, specificity)
melted <- melt(df_res)
melted %>%
  ggplot(mapping = aes(x = type, y = value, fill = variable)) + 
  geom_col(position = 'dodge') + 
  labs(x = "Method", y = "Accuracy") +
  scale_fill_manual("Accuracy Metric", values = c("#2274AE", "goldenrod2")) + 
  theme_classic() +
  geom_text(aes(label = round(value, 2)), vjust = -0.4, position = position_dodge(0.9)) +
  annotate("text", x = 1:3, y = 1.15, label = vars, fontface = "bold")
```


Our chosen model is a simple decision tree. This model achieves a **98% sensitivity rate** (predicting a CSI when the patient truly has CSI) and a **75% specificity rate** (predicting no CSI when that is truly the case). \red{(I need exact values here.)} Thus, we achieve over three times the specificity of @leonard2011factors while maintaining the same sensitivity. Statistically speaking, we increase the power over threefold without sacrificing the potential for a Type I error. 


Prior to discovering this model, we fit a decision tree with the subset of features obtained in Section \blue{\ref{sec:feat-sel}}. This decision tree achieved a higher specificity rate of roughly 80%. However, this model may not be valid in a clinical context due to certain factors that influence the reporting of a certain feature. Our initial model has a domain-specific caveats that we must adjust for and address.

First, the `DxCspineInjury` feature (explained in Section \blue{\ref{sec:corr-out}}) has strong predictive power. This is likely due to the fact that this predictor is literally a guess as to whether this person has a CSI or if they arrived with a diagnosis. This information could be revealing of the true outcome even though it is a feature made available to a physician prior to making the CDR. A rigorous inspection of the raw data reveals that this feature has **zero** missing or indeterminate values. Therefore, we use this variable can be utilized in a real-world context. We discuss this concept further in Section \blue{\ref{sec:stability}}.

Second, some features in our data are unobservable based on the value of other features. In the case of our initial model, the `PtSensoryLoss` feature measures whether the patient 

Subject matter expertise tells us that



# Comparability

\red{Are the data units comparable or normalized so that they can be treated as if they were exchangeable? Or are apples and oranges being combined?}

\red{Are the data units independent? Are two columns of data duplicates of the same variable?}


# Visualization

\red{Look at data or subsets of them. Create plots of 1 and 2 dimensional data. Examine summaries of such data. What are the ranges? Do they make sense? Are there any missing values? Use color and dynamic plots. Is anything unexpected? It is worth noting that 30 percent of our cortex is devoted to vision, so visualization is highly effective to discover patterns and unusual things in data. Often, to bring out patterns in big data, visualization is most useful after some model building, for example, to obtain residuals to visualize.}


# Randomness

\red{Statistical inference concepts such as p-values and confidence intervals rely on randomness. What does randomness mean in the data? Make the randomness in the statistical model as explicit as possible. What domain knowledge supports such a statistical or mathematical abstraction or the randomness in a statistical model?}

\red{What is the randomness in this PECARN data set? Is it a random sample from a population? Which one? Why can the data be viewed as a random sample? What assumptions are being made? Can one check these conditions using the info on the data collection process?}


# Stability {#sec:stability}

\red{What off-the-shelf method will you use? Do different methods give the same qualitative conclusion? Perturb oneâ€™s data, for example, by adding noise or subsampling if data units are exchangeable (in general, make sure the subsamples respect the underlying structures, e.g. dependence, clustering, heterogeneity, so the subsamples are representative of the original data). Do the conclusions still hold? Only trust those that pass the stability test, which is an easy-to-implement, first defense against over-fitting or too many false positive discoveries.}

# Acknowledgments

The authors would like to thank Dr. Michael Boyle from the UC San Francisco School of Medicine for his expertise and extremely helpful advice in determining which judgment calls to make. This project would not have been grounded in reality without Dr. Boyle's assistance.

The authors would also like to thank Chandan Singh from the Yu Group for setting up this project and for providing streamlined modeling templates for us to use.

Finally, the authors would like to express their gratitude to the STAT 215A teaching team, Dr. Bin Yu and Omer Ronen, for their wisdom, support and encouragement throughout the semester. We feel comfortable tackling complex research issues like these because of what we learned in lecture and discussion. These lessons will serve us well throughout our entire PhD.

# References





