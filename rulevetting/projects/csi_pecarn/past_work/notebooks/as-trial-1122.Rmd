---
title: An R Markdown document converted from "/Users/andyshen/Library/Mobile Documents/com~apple~CloudDocs/Desktop/Desktop
  - Andyâ€™s MacBook Pro/UC Berkeley/STAT 215A/rule-vetting/rulevetting/projects/csi_pecarn/notebooks/as-trial-1122.ipynb"
output: html_document
---

```{python}
# %load_ext autoreload
# %autoreload 2

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

import rulevetting.api.viz as viz
from rulevetting.projects.csi_pecarn.dataset import Dataset # are we still importing from csi?

outcome_def = 'outcome'  # output
# %matplotlib inline
# %load_ext autoreload
# %autoreload 2
```

```{python}
import rulevetting
import os
from os.path import join as oj
from tqdm import tqdm
from typing import Dict

print(f'{"DATA_PATH:":>15}', rulevetting.DATA_PATH)
print(f'{"REPO_PATH:":>15}', rulevetting.REPO_PATH)
print(f'{"MRULES_PATH:":>15}', rulevetting.MRULES_PATH)
print(f'{"PROJECTS_PATH:":>15}', rulevetting.PROJECTS_PATH)
```

# Briefly check data

```{python}
data_path: str = rulevetting.DATA_PATH
raw_data_path = oj(data_path, "csi_pecarn", 'raw')

fnames = sorted([
    fname for fname in os.listdir(raw_data_path)
    if 'csv' in fname
    #   and not 'radiology' in fname
    #   and not 'kappa' in fname
    ])  # remove outcome (SH: not yet)

r = {}
print('read all the csvs...', fnames)
if len(fnames) == 0:
    print('no csvs found in path', raw_data_path)
for fname in tqdm(fnames):
    df = pd.read_csv(oj(raw_data_path, fname), encoding="ISO-8859-1")
    df.rename(columns={'site': 'SiteID'}, inplace=True)
    df.rename(columns={'SITE': 'SiteID'}, inplace=True)
    df.rename(columns={'caseid': 'CaseID'}, inplace=True)
    df.rename(columns={'controltype': 'ControlType'}, inplace=True)
    df.rename(columns={'studysubjectid': 'SubjectID'}, inplace=True)
    df.rename(columns={'StudySubjectID': 'SubjectID'}, inplace=True)
    assert ('SiteID' in df.keys())
    assert ('CaseID' in df.keys())
    assert ('ControlType' in df.keys())
    assert ('SubjectID' in df.keys())
    r[fname] = df
```

```{python}
print(fname)
fnames
```

```{python}
print(r.keys())
for fname in fnames:
    df = r[fname]
    print(f'{fname:>35}', df.shape)
```

injuryclassfication / kappa / radiologyreview : less rows than others

```{python}
print("Check unique SubjectID for each .csv")
a = 0
for fname in fnames:
    df = r[fname]
    if df.shape[0] != len(pd.unique(df.SubjectID)):
        a += 1
        print(fname)
if(a==0):
    print("ALL .csv file has unique SubjectID")
```

```{python}
fnames_small = [fname for fname in fnames
                if not 'radiology' in fname
                    and not 'kappa' in fname
                    and not 'injuryclassification' in fname
                    and not 'outside' in fname
                    and not 'onfield' in fname
                    and not 'medicalhistory' in fname]
print(fnames_small)

cols = 0
rows = 0

df_features = r[fnames_small[0]]
print('merge all the dfs...')
for i, fname in tqdm(enumerate(fnames_small)):
    df2 = r[fname].copy()
    # if subj has multiple entries, only keep first
    df2 = df2.drop_duplicates(subset=['SubjectID'], keep='last')
    # don't save duplicate columns
    df_features = df_features.set_index('SubjectID').combine_first(df2.set_index('SubjectID')).reset_index()
    # SH: check the number ofcolumns
    rows += df2.shape[0]
    cols += df2.shape[1]
```

```{python}
print(rows, cols)
print(len(df_features.columns))
print(len(pd.unique(df_features.columns)))
print(df_features.shape)
```

The difference between `cols` and `df_features.shape[1]` = 4 * (`len(fnames_small)`-1) 
(because of `SiteID`, `CaseID`, `ControlType`, `SubjectID`)

```{python}
# SH: NO outcomes yet
# df_outcomes = helper.get_outcomes(raw_data_path)
```

```{python}
df = df_features # SH: not working - some variables must be constant
#df = r[fnames[0]]
processed_feats = df.keys().values.tolist()
print(len(processed_feats))
```

```{python}
temp = df.var(axis = 0, skipna = True, numeric_only = True) == 0
const_var = []
for i in temp.index:
    if temp[i]:
#        print(i)
        const_var.append(i)
print(const_var)
```

```{python}
df[const_var]
```

```{python}
print(df["ControlType"].unique())
df = df.assign(outcome=lambda x: (x.ControlType == 'case').astype(int))
df.drop(columns = "ControlType")
```

```{python}
#print(df.dtypes)
dc = {}
for t in df.dtypes:
    if t in dc.keys():
        dc[t] = dc[t] +1
    else:
        dc[t] = 1
print(dc)
```

There is no Boolean column

### Choose only valid(numeric) columns

```{python}
valids = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
newdf = df.select_dtypes(include=valids)
```

```{python}
newdf
```

### Check invalid columns

```{python}
invalid_columns = df.columns[df.dtypes == "object"]
print(len(invalid_columns))
print(invalid_columns)
```

```{python}
i = 0
for c in invalid_columns:
    temp = df.select_dtypes(include="object")[c].unique()
    if len(temp) <= 30:
        #print(c)
        #print(temp, "\n")
        i += 1
print(i)
```

```{python}
# SH: not included in "newdf" yet
additional_valid_columns = ["AVPU",  "ArrPtIntub", "DxCspineInjury", "Ethnicity", "Gender",
                            "HeadFirst", "HeadFirstRegion", "InjuryPrimaryMechanism", "IntervForCervicalStab", "LimitedRangeMotion",
                            "LongTermRehab", "MedsGiven", "MedsRecdPriorArrival", "MinorInjuries", "PtAmbulatoryPriorArrival",
                            "PtCompPain", "PtCompPainNeckMove", "PtExtremityWeakness", "PtParesthesias", "PtSensoryLoss",
                            "PtTender", "TotalGCS", "clotheslining", "helmet"]
# 34 variables
```

### Plot correlation heatmap

```{python}
nonconst_var = np.setdiff1d(np.asarray(newdf.columns), const_var)
print(len(np.asarray(newdf.columns)))
print(len(const_var))
print(len(nonconst_var))
newdf = newdf[nonconst_var]
```

```{python}
# pandas impute missing values with median
newdf = newdf.fillna(df.median(numeric_only = True))
print(len(newdf.columns))
```

```{python}
#corrs = df[processed_feats].corr(method='pearson')
corrs = newdf[nonconst_var].corr(method='pearson')
print(corrs.shape)
#print(corrs)
```

```{python}
corrs.isna().sum().sum()
```

```{python}
corrmap = sns.clustermap(corrs, cmap=viz.cmap_div, vmin=-1, vmax=1, cbar_kws={'label': 'Correlation'},
#                         yticklabels=True,
                         figsize=(25, 25))
corrmap.ax_row_dendrogram.set_visible(False)
corrmap.ax_col_dendrogram.set_visible(False)
```

### Variables to check for helper.py

```{python}
temp = newdf.columns.tolist()
len(temp)
temp.extend(additional_valid_columns)
temp = sorted(temp)
# print(len(temp)) # 159
c = len(temp) // 3
print(len(temp), len(temp[:c]) + len(temp[c:2*c]) + len(temp[2*c:]), "\n-----")
print("Andy: ", temp[:c], "\n-----")
print("Licong: ", temp[c: 2*c], "\n-----")
print("SH: ", temp[2*c:], "\n-----")
```

## Here is the end of SH progress (11/22/2021)
***

# get data

```{python}
df_train, df_tune, _ = Dataset().get_data(load_csvs=True)
df = df_train
processed_feats = df.keys().values.tolist()
print(processed_feats)
print(len(processed_feats))
```

**correlations between features**

```{python}
# corrs = df[feat_names + [outcome_def]].corr()
corrs = df[processed_feats].corr(method='pearson')
corrmap = sns.clustermap(corrs, cmap=viz.cmap_div, vmin=-1, vmax=1, cbar_kws={'label': 'Correlation'},
                         yticklabels=True,
                         figsize=(15, 15))
corrmap.ax_row_dendrogram.set_visible(False)
corrmap.ax_col_dendrogram.set_visible(False)

plt.show()
# plt.savefig('results/corrplot.png', dpi=300, bbox_inches='tight')
```

**individual correlations with outcome**

```{python}
plt.figure(dpi=250, figsize=(4, 7))
vals = corrs[outcome_def]
args = np.argsort(vals)
labs = vals.index.values[args]
ax = plt.subplot(111)
plt.barh(labs[:-1], vals[args][:-1])
plt.xlabel('Correlation w/ outcome')
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
plt.show()
```

**subgroup risks (with sizes)**

```{python}
def plot_subgroup_risks(df, head=None, tail=None):
    plt.figure(dpi=250, figsize=(4, 7))
    ks = np.array([k for k in df.keys()
                   if not k == outcome_def])
    mean_risk_baseline = df[outcome_def].mean() * 100
    if head is not None or tail is not None:
        ks = np.array([k for k in ks
                       if np.sum(df[k]) >= 10
                       and np.abs(np.mean(df[k]) - mean_risk_baseline) > 1
                       and not np.sum([df[k] == 1]) == 0])  # should have at least one IAI-I
    vals = np.array([np.mean(df[outcome_def][df[k] == 1]) for k in ks])
    vals[np.isnan(vals)] = mean_risk_baseline
    counts = np.array([np.sum([df[k] == 1]) for k in ks])
    args = np.argsort(vals)
    if head is not None:
        args = args[:head]
    if tail is not None:
        args = args[-tail:]
    labs = ks[args]
    vals = vals[args] * 100
    counts = counts[args]
    counts_norm = counts / np.nanmax(counts)
    ax = plt.subplot(111)
    plt.plot(vals, np.arange(vals.size), 'o', ms=1)
    plt.barh(y=np.arange(vals.size), width=vals, height=1.0 * counts_norm)
    plt.yticks(np.arange(vals.size), [lab.replace('_yes', '') for lab in labs])
    for i, (v, c) in enumerate(zip(vals, counts)):
        ax.text(v, i - 0.1, str(c), color=viz.cg, fontweight='bold')
    plt.axvline(mean_risk_baseline, color='gray', linestyle='--', alpha=0.5)
    ax.text(vals.max(), 1, 'Count in subgroup', color=viz.cg, fontweight='bold')
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    plt.xlabel('IAI-I risk (%)')
    plt.show()


# plot_subgroup_risks(df[processed_feats + [outcome_def]])
# dd = df[(df['AbdTenderDegree_None']==1) & (df['GCSScore_Full_yes'] == 1)] # low-risk group left to split
# plot_subgroup_risks(dd[[k for k in processed_feats if not k in ['AbdTenderDegree_None', 'GCSScore_Full_yes']] + [outcome_def]])
dd = df[(df['AbdTenderDegree_None'] == 1) & (
            df['GCSScore_Full_yes'] == 1)]  # & (df['DecrBreathSound_yes'] == 0)] # low-risk group left to split
plot_subgroup_risks(dd[[k for k in processed_feats
                        if not k in ['AbdTenderDegree_None', 'GCSScore_Full_yes']]])
```

**joint correlations (or risks) with outcome joint**

```{python}
func = lambda x, y: np.corrcoef(x, y)[0, 1]  # corr
# func = lambda x, y: np.mean(y[x]) # frac high-risk
d = len(processed_feats)
mat = np.zeros((2 * d, 2 * d))
y = df[outcome_def]
for r in range(d):
    for c in range(d):
        xr = df[processed_feats[r]] == 1
        xc = df[processed_feats[c]] == 1
        mat[2 * r, 2 * c] = func(xr & xc, y)
        mat[2 * r + 1, 2 * c] = func(~xr & xc, y)
        mat[2 * r, 2 * c + 1] = func(xr & ~xc, y)
        mat[2 * r + 1, 2 * c + 1] = func(~xr & ~xc, y)

fs = np.array([[x, '~' + x] for x in processed_feats]).flatten()
mat[np.isnan(mat)] = 0
matt = pd.DataFrame(mat, index=fs, columns=fs)
vabs = np.max([np.abs(np.min(mat)), np.max(mat)])
vmin = -vabs
cmap = viz.cmap_div
if np.min(mat) >= 0:
    vmin = 0
    cmap = 'Blues'
corrmap = sns.clustermap(matt, cmap=cmap, vmin=vmin, vmax=vabs,
                         dendrogram_ratio=0.2, figsize=(10, 10), )
corrmap.ax_row_dendrogram.set_visible(False)
corrmap.ax_col_dendrogram.set_visible(False)
#                row_cluster=False, col_cluster=False)
# plt.savefig('dendrogram.png', dpi=400)
# plt.tight_layout()
plt.show()
```

# features scatter plots

## continuous features

```{python}
feats_numerical = ['GCSScore', 'Age']
# feats_categorical = ['AbdTenderDegree', 'Race', 'MOI']
R, C, = 1, 2
plt.figure(figsize=(6, 3), dpi=500)
for i in range(len(feats_numerical)):
    plt.subplot(R, C, i + 1)
    feat = feats_numerical[i]
    plt.hist(df[df[outcome_def] == 1][feat], density=True, alpha=1, label='pos', color=viz.cb)
    plt.hist(df[df[outcome_def] == 0][feat], density=True, alpha=0.5, label='neg', color=viz.cr)
    plt.xlabel(feat, fontsize=14)
plt.legend()
plt.tight_layout()
plt.show()
```

**we can cut GCSScore as whether it is 15 or not (14 is already pretty bad)**

```{python}
feat = 'GCSScore'
print(np.unique(df[df[outcome_def] == 1][feat], return_counts=True))
print(np.unique(df[df[outcome_def] == 0][feat], return_counts=True))
```

## scatter plots

```{python}
viz.jointplot_grouped('Age', 'GCSScore', 'outcome', df)
```

```{python}
viz.jointplot_grouped('Age', 'Hypotension_yes', 'outcome', df)
```

