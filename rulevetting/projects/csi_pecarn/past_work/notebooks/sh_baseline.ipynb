{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit interpretable models to the training set and test on validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyshen/Library/Mobile Documents/com~apple~CloudDocs/Desktop/Desktop - Andyâ€™s MacBook Pro/UC Berkeley/STAT 215A/rule-env/lib/python3.8/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle as pkl\n",
    "from os.path import join as oj\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import imodels\n",
    "from rulevetting.api import validation_new\n",
    "from rulevetting.projects.csi_pecarn.dataset_sh_1206 import Dataset\n",
    "\n",
    "MODELS_DIR = './models'\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "outcome_def = 'outcome'  # output\n",
    "\n",
    "from rulevetting.projects.csi_pecarn.model_helper import var_selection, predict_and_save, model_valid, fit_simple_models, fit_other_models, plot_metrics, print_metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1988, 45)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_tune, df_test = Dataset().get_data(load_csvs=True)\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns=outcome_def)\n",
    "y_train = df_train[outcome_def].values\n",
    "X_tune = df_tune.drop(columns=outcome_def)\n",
    "y_tune = df_tune[outcome_def].values\n",
    "X_test = df_test.drop(columns=outcome_def)\n",
    "y_test = df_test[outcome_def].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_baseline_model(X, y,\n",
    "                         original_8_vars = ['AlteredMentalStatus', 'FocalNeuroFindings', 'PainNeck', 'SubInj_TorsoTrunk', 'Torticollis',\n",
    "                                            'Predisposed', 'HighriskDiving', 'HighriskMVC']\n",
    "                        ):\n",
    "    \n",
    "    # setting\n",
    "    data = X[original_8_vars]\n",
    "    n = data.shape[0]\n",
    "\n",
    "    # construct a df\n",
    "    check = pd.DataFrame()\n",
    "    check[\"real\"] = y.astype(str)\n",
    "    pred = np.array((data[original_8_vars].sum(axis = 1) != 0).astype(int).astype(str))\n",
    "    check[\"pred\"] = pred\n",
    "    \n",
    "    # count & calculate sens. and spec.\n",
    "    non_csi = check.groupby([\"real\"]).size()[\"0\"]\n",
    "    csi = check.groupby([\"real\"]).size()[\"1\"]\n",
    "    # print(\"total:\", n, \"/ csi:\", csi, \"/ non_csi:\", non_csi)\n",
    "    counts = check.groupby([\"real\", \"pred\"]).size()\n",
    "    sens = counts[\"1\"][\"1\"] / csi\n",
    "    spec = counts[\"0\"][\"0\"] / non_csi\n",
    "    # print(\"sens: \", sens, \"/ spec:\", spec)\n",
    "    \n",
    "    return sens, spec, n, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_result = {'train':{}, 'tune':{}, 'test':{}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y, suffix in zip([X_train, X_tune, X_test],\n",
    "                        [y_train, y_tune, y_test],\n",
    "                        ['train', 'tune', 'test']):\n",
    "    sens, spec, n, counts = check_baseline_model(x, y)\n",
    "    baseline_result[suffix][\"sensitivity\"] = sens\n",
    "    baseline_result[suffix][\"specificity\"] = spec\n",
    "    baseline_result[suffix][\"total patients\"] = n\n",
    "    baseline_result[suffix][\"counts\"] = counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train /// sens:  0.9029126213592233 / spec: 0.42167957117331745\n",
      "tune /// sens:  0.9439252336448598 / spec: 0.3902877697841727\n",
      "test /// sens:  0.9435483870967742 / spec: 0.3877551020408163\n"
     ]
    }
   ],
   "source": [
    "for suffix in ['train', 'tune', 'test']:\n",
    "    print(suffix, \"/// sens: \", baseline_result[suffix][\"sensitivity\"], \"/ spec:\", baseline_result[suffix][\"specificity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rule-env",
   "language": "python",
   "name": "rule-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
