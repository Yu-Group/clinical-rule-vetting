<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>rulevetting.templates.dataset API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>rulevetting.templates.dataset</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
import random
from abc import abstractmethod
from os.path import join as oj
from typing import Dict

import numpy as np
import pandas as pd
from joblib import Memory

import rulevetting
from vflow import init_args, Vset, build_Vset


class DatasetTemplate:
    &#34;&#34;&#34;Classes that use this template should be called &#34;Dataset&#34;
    All functions take **kwargs, so you can specify any judgement calls you aren&#39;t sure about with a kwarg flag.
    Please refrain from shuffling / reordering the data in any of these functions, to ensure a consistent test set.
    &#34;&#34;&#34;

    @abstractmethod
    def clean_data(self, data_path: str = rulevetting.DATA_PATH, **kwargs) -&gt; pd.DataFrame:
        &#34;&#34;&#34;
        Convert the raw data files into a pandas dataframe.
        Dataframe keys should be reasonable (lowercase, underscore-separated).
        Data types should be reasonable.

        Params
        ------
        data_path: str, optional
            Path to all data files
        kwargs: dict
            Dictionary of hyperparameters specifying judgement calls

        Returns
        -------
        cleaned_data: pd.DataFrame
        &#34;&#34;&#34;
        return NotImplemented

    @abstractmethod
    def preprocess_data(self, cleaned_data: pd.DataFrame, **kwargs) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Preprocess the data.
        Impute missing values.
        Scale/transform values.
        Should put the prediction target in a column named &#34;outcome&#34;

        Parameters
        ----------
        cleaned_data: pd.DataFrame
        kwargs: dict
            Dictionary of hyperparameters specifying judgement calls

        Returns
        -------
        preprocessed_data: pd.DataFrame
        &#34;&#34;&#34;
        return NotImplemented

    @abstractmethod
    def extract_features(self, preprocessed_data: pd.DataFrame, **kwargs) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Extract features from preprocessed data
        All features should be binary


        Parameters
        ----------
        preprocessed_data: pd.DataFrame
        kwargs: dict
            Dictionary of hyperparameters specifying judgement calls

        Returns
        -------
        extracted_features: pd.DataFrame
        &#34;&#34;&#34;
        return NotImplemented

    def split_data(self, preprocessed_data: pd.DataFrame) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Split into 3 sets: training, tuning, testing.
        Do not modify (to ensure consistent test set).
        Keep in mind any natural splits (e.g. hospitals).
        Ensure that there are positive points in all splits.

        Parameters
        ----------
        preprocessed_data
        kwargs: dict
            Dictionary of hyperparameters specifying judgement calls

        Returns
        -------
        df_train
        df_tune
        df_test
        &#34;&#34;&#34;
        return tuple(np.split(
            preprocessed_data.sample(frac=1, random_state=42),
            [int(.6 * len(preprocessed_data)),  # 60% train
             int(.8 * len(preprocessed_data))]  # 20% tune, 20% test
        ))

    @abstractmethod
    def get_outcome_name(self) -&gt; str:
        &#34;&#34;&#34;Should return the name of the outcome we are predicting
        &#34;&#34;&#34;
        return NotImplemented

    @abstractmethod
    def get_dataset_id(self) -&gt; str:
        &#34;&#34;&#34;Should return the name of the dataset id (str)
        &#34;&#34;&#34;
        return NotImplemented

    @abstractmethod
    def get_meta_keys(self) -&gt; list:
        &#34;&#34;&#34;Return list of keys which should not be used in fitting but are still useful for analysis
        &#34;&#34;&#34;
        return NotImplemented

    def get_judgement_calls_dictionary(self) -&gt; Dict[str, Dict[str, list]]:
        &#34;&#34;&#34;Return dictionary of keyword arguments for each function in the dataset class.
        Each key should be a string with the name of the arg.
        Each value should be a list of values, with the default value coming first.

        Example
        -------
        return {
            &#39;clean_data&#39;: {},
            &#39;preprocess_data&#39;: {
                &#39;imputation_strategy&#39;: [&#39;mean&#39;, &#39;median&#39;],  # first value is default
            },
            &#39;extract_features&#39;: {},
        }
        &#34;&#34;&#34;
        return NotImplemented

    def get_data(self, save_csvs: bool = False,
                 data_path: str = rulevetting.DATA_PATH,
                 load_csvs: bool = False,
                 run_perturbations: bool = False) -&gt; (pd.DataFrame, pd.DataFrame, pd.DataFrame):
        &#34;&#34;&#34;Runs all the processing and returns the data.
        This method does not need to be overriden.

        Params
        ------
        save_csvs: bool, optional
            Whether to save csv files of the processed data
        data_path: str, optional
            Path to all data
        load_csvs: bool, optional
            Whether to skip all processing and load data directly from csvs
        run_perturbations: bool, optional
            Whether to run / save data pipeline for all combinations of judgement calls

        Returns
        -------
        df_train
        df_tune
        df_test
        &#34;&#34;&#34;
        PROCESSED_PATH = oj(data_path, self.get_dataset_id(), &#39;processed&#39;)
        if load_csvs:
            return tuple([pd.read_csv(oj(PROCESSED_PATH, s), index_col=0)
                          for s in [&#39;train.csv&#39;, &#39;tune.csv&#39;, &#39;test.csv&#39;]])
        np.random.seed(0)
        random.seed(0)
        CACHE_PATH = oj(data_path, &#39;joblib_cache&#39;)
        cache = Memory(CACHE_PATH, verbose=0).cache
        kwargs = self.get_judgement_calls_dictionary()
        default_kwargs = {}
        for key in kwargs.keys():
            func_kwargs = kwargs[key]
            default_kwargs[key] = {k: func_kwargs[k][0]  # first arg in each list is default
                                   for k in func_kwargs.keys()}

        print(&#39;kwargs&#39;, default_kwargs)
        if not run_perturbations:
            cleaned_data = cache(self.clean_data)(data_path=data_path, **default_kwargs[&#39;clean_data&#39;])
            preprocessed_data = cache(self.preprocess_data)(cleaned_data, **default_kwargs[&#39;preprocess_data&#39;])
            extracted_features = cache(self.extract_features)(preprocessed_data, **default_kwargs[&#39;extract_features&#39;])
            df_train, df_tune, df_test = cache(self.split_data)(extracted_features)
        elif run_perturbations:
            data_path_arg = init_args([data_path], names=[&#39;data_path&#39;])[0]
            clean_set = build_Vset(&#39;clean_data&#39;, self.clean_data, param_dict=kwargs[&#39;clean_data&#39;], cache_dir=CACHE_PATH)
            cleaned_data = clean_set(data_path_arg)
            preprocess_set = build_Vset(&#39;preprocess_data&#39;, self.preprocess_data, param_dict=kwargs[&#39;preprocess_data&#39;],
                                        cache_dir=CACHE_PATH)
            preprocessed_data = preprocess_set(cleaned_data)
            extract_set = build_Vset(&#39;extract_features&#39;, self.extract_features, param_dict=kwargs[&#39;extract_features&#39;],
                                     cache_dir=CACHE_PATH)
            extracted_features = extract_set(preprocessed_data)
            split_data = Vset(&#39;split_data&#39;, modules=[self.split_data])
            dfs = split_data(extracted_features)
        if save_csvs:
            os.makedirs(PROCESSED_PATH, exist_ok=True)

            if not run_perturbations:
                for df, fname in zip([df_train, df_tune, df_test],
                                     [&#39;train.csv&#39;, &#39;tune.csv&#39;, &#39;test.csv&#39;]):
                    meta_keys = rulevetting.api.util.get_feat_names_from_base_feats(df.keys(), self.get_meta_keys())
                    df.loc[:, meta_keys].to_csv(oj(PROCESSED_PATH, f&#39;meta_{fname}&#39;))
                    df.drop(columns=meta_keys).to_csv(oj(PROCESSED_PATH, fname))
            if run_perturbations:
                for k in dfs.keys():
                    if isinstance(k, tuple):
                        os.makedirs(oj(PROCESSED_PATH, &#39;perturbed_data&#39;), exist_ok=True)
                        perturbation_name = str(k).replace(&#39;, &#39;, &#39;_&#39;).replace(&#39;(&#39;, &#39;&#39;).replace(&#39;)&#39;, &#39;&#39;)
                        perturbed_path = oj(PROCESSED_PATH, &#39;perturbed_data&#39;, perturbation_name)
                        os.makedirs(perturbed_path, exist_ok=True)
                        for i, fname in enumerate([&#39;train.csv&#39;, &#39;tune.csv&#39;, &#39;test.csv&#39;]):
                            df = dfs[k][i]
                            meta_keys = rulevetting.api.util.get_feat_names_from_base_feats(df.keys(),
                                                                                            self.get_meta_keys())
                            df.loc[:, meta_keys].to_csv(oj(perturbed_path, f&#39;meta_{fname}&#39;))
                            df.drop(columns=meta_keys).to_csv(oj(perturbed_path, fname))
                return dfs[list(dfs.keys())[0]]

        return df_train, df_tune, df_test</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="rulevetting.templates.dataset.DatasetTemplate"><code class="flex name class">
<span>class <span class="ident">DatasetTemplate</span></span>
</code></dt>
<dd>
<div class="desc"><p>Classes that use this template should be called "Dataset"
All functions take **kwargs, so you can specify any judgement calls you aren't sure about with a kwarg flag.
Please refrain from shuffling / reordering the data in any of these functions, to ensure a consistent test set.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DatasetTemplate:
    &#34;&#34;&#34;Classes that use this template should be called &#34;Dataset&#34;
    All functions take **kwargs, so you can specify any judgement calls you aren&#39;t sure about with a kwarg flag.
    Please refrain from shuffling / reordering the data in any of these functions, to ensure a consistent test set.
    &#34;&#34;&#34;

    @abstractmethod
    def clean_data(self, data_path: str = rulevetting.DATA_PATH, **kwargs) -&gt; pd.DataFrame:
        &#34;&#34;&#34;
        Convert the raw data files into a pandas dataframe.
        Dataframe keys should be reasonable (lowercase, underscore-separated).
        Data types should be reasonable.

        Params
        ------
        data_path: str, optional
            Path to all data files
        kwargs: dict
            Dictionary of hyperparameters specifying judgement calls

        Returns
        -------
        cleaned_data: pd.DataFrame
        &#34;&#34;&#34;
        return NotImplemented

    @abstractmethod
    def preprocess_data(self, cleaned_data: pd.DataFrame, **kwargs) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Preprocess the data.
        Impute missing values.
        Scale/transform values.
        Should put the prediction target in a column named &#34;outcome&#34;

        Parameters
        ----------
        cleaned_data: pd.DataFrame
        kwargs: dict
            Dictionary of hyperparameters specifying judgement calls

        Returns
        -------
        preprocessed_data: pd.DataFrame
        &#34;&#34;&#34;
        return NotImplemented

    @abstractmethod
    def extract_features(self, preprocessed_data: pd.DataFrame, **kwargs) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Extract features from preprocessed data
        All features should be binary


        Parameters
        ----------
        preprocessed_data: pd.DataFrame
        kwargs: dict
            Dictionary of hyperparameters specifying judgement calls

        Returns
        -------
        extracted_features: pd.DataFrame
        &#34;&#34;&#34;
        return NotImplemented

    def split_data(self, preprocessed_data: pd.DataFrame) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Split into 3 sets: training, tuning, testing.
        Do not modify (to ensure consistent test set).
        Keep in mind any natural splits (e.g. hospitals).
        Ensure that there are positive points in all splits.

        Parameters
        ----------
        preprocessed_data
        kwargs: dict
            Dictionary of hyperparameters specifying judgement calls

        Returns
        -------
        df_train
        df_tune
        df_test
        &#34;&#34;&#34;
        return tuple(np.split(
            preprocessed_data.sample(frac=1, random_state=42),
            [int(.6 * len(preprocessed_data)),  # 60% train
             int(.8 * len(preprocessed_data))]  # 20% tune, 20% test
        ))

    @abstractmethod
    def get_outcome_name(self) -&gt; str:
        &#34;&#34;&#34;Should return the name of the outcome we are predicting
        &#34;&#34;&#34;
        return NotImplemented

    @abstractmethod
    def get_dataset_id(self) -&gt; str:
        &#34;&#34;&#34;Should return the name of the dataset id (str)
        &#34;&#34;&#34;
        return NotImplemented

    @abstractmethod
    def get_meta_keys(self) -&gt; list:
        &#34;&#34;&#34;Return list of keys which should not be used in fitting but are still useful for analysis
        &#34;&#34;&#34;
        return NotImplemented

    def get_judgement_calls_dictionary(self) -&gt; Dict[str, Dict[str, list]]:
        &#34;&#34;&#34;Return dictionary of keyword arguments for each function in the dataset class.
        Each key should be a string with the name of the arg.
        Each value should be a list of values, with the default value coming first.

        Example
        -------
        return {
            &#39;clean_data&#39;: {},
            &#39;preprocess_data&#39;: {
                &#39;imputation_strategy&#39;: [&#39;mean&#39;, &#39;median&#39;],  # first value is default
            },
            &#39;extract_features&#39;: {},
        }
        &#34;&#34;&#34;
        return NotImplemented

    def get_data(self, save_csvs: bool = False,
                 data_path: str = rulevetting.DATA_PATH,
                 load_csvs: bool = False,
                 run_perturbations: bool = False) -&gt; (pd.DataFrame, pd.DataFrame, pd.DataFrame):
        &#34;&#34;&#34;Runs all the processing and returns the data.
        This method does not need to be overriden.

        Params
        ------
        save_csvs: bool, optional
            Whether to save csv files of the processed data
        data_path: str, optional
            Path to all data
        load_csvs: bool, optional
            Whether to skip all processing and load data directly from csvs
        run_perturbations: bool, optional
            Whether to run / save data pipeline for all combinations of judgement calls

        Returns
        -------
        df_train
        df_tune
        df_test
        &#34;&#34;&#34;
        PROCESSED_PATH = oj(data_path, self.get_dataset_id(), &#39;processed&#39;)
        if load_csvs:
            return tuple([pd.read_csv(oj(PROCESSED_PATH, s), index_col=0)
                          for s in [&#39;train.csv&#39;, &#39;tune.csv&#39;, &#39;test.csv&#39;]])
        np.random.seed(0)
        random.seed(0)
        CACHE_PATH = oj(data_path, &#39;joblib_cache&#39;)
        cache = Memory(CACHE_PATH, verbose=0).cache
        kwargs = self.get_judgement_calls_dictionary()
        default_kwargs = {}
        for key in kwargs.keys():
            func_kwargs = kwargs[key]
            default_kwargs[key] = {k: func_kwargs[k][0]  # first arg in each list is default
                                   for k in func_kwargs.keys()}

        print(&#39;kwargs&#39;, default_kwargs)
        if not run_perturbations:
            cleaned_data = cache(self.clean_data)(data_path=data_path, **default_kwargs[&#39;clean_data&#39;])
            preprocessed_data = cache(self.preprocess_data)(cleaned_data, **default_kwargs[&#39;preprocess_data&#39;])
            extracted_features = cache(self.extract_features)(preprocessed_data, **default_kwargs[&#39;extract_features&#39;])
            df_train, df_tune, df_test = cache(self.split_data)(extracted_features)
        elif run_perturbations:
            data_path_arg = init_args([data_path], names=[&#39;data_path&#39;])[0]
            clean_set = build_Vset(&#39;clean_data&#39;, self.clean_data, param_dict=kwargs[&#39;clean_data&#39;], cache_dir=CACHE_PATH)
            cleaned_data = clean_set(data_path_arg)
            preprocess_set = build_Vset(&#39;preprocess_data&#39;, self.preprocess_data, param_dict=kwargs[&#39;preprocess_data&#39;],
                                        cache_dir=CACHE_PATH)
            preprocessed_data = preprocess_set(cleaned_data)
            extract_set = build_Vset(&#39;extract_features&#39;, self.extract_features, param_dict=kwargs[&#39;extract_features&#39;],
                                     cache_dir=CACHE_PATH)
            extracted_features = extract_set(preprocessed_data)
            split_data = Vset(&#39;split_data&#39;, modules=[self.split_data])
            dfs = split_data(extracted_features)
        if save_csvs:
            os.makedirs(PROCESSED_PATH, exist_ok=True)

            if not run_perturbations:
                for df, fname in zip([df_train, df_tune, df_test],
                                     [&#39;train.csv&#39;, &#39;tune.csv&#39;, &#39;test.csv&#39;]):
                    meta_keys = rulevetting.api.util.get_feat_names_from_base_feats(df.keys(), self.get_meta_keys())
                    df.loc[:, meta_keys].to_csv(oj(PROCESSED_PATH, f&#39;meta_{fname}&#39;))
                    df.drop(columns=meta_keys).to_csv(oj(PROCESSED_PATH, fname))
            if run_perturbations:
                for k in dfs.keys():
                    if isinstance(k, tuple):
                        os.makedirs(oj(PROCESSED_PATH, &#39;perturbed_data&#39;), exist_ok=True)
                        perturbation_name = str(k).replace(&#39;, &#39;, &#39;_&#39;).replace(&#39;(&#39;, &#39;&#39;).replace(&#39;)&#39;, &#39;&#39;)
                        perturbed_path = oj(PROCESSED_PATH, &#39;perturbed_data&#39;, perturbation_name)
                        os.makedirs(perturbed_path, exist_ok=True)
                        for i, fname in enumerate([&#39;train.csv&#39;, &#39;tune.csv&#39;, &#39;test.csv&#39;]):
                            df = dfs[k][i]
                            meta_keys = rulevetting.api.util.get_feat_names_from_base_feats(df.keys(),
                                                                                            self.get_meta_keys())
                            df.loc[:, meta_keys].to_csv(oj(perturbed_path, f&#39;meta_{fname}&#39;))
                            df.drop(columns=meta_keys).to_csv(oj(perturbed_path, fname))
                return dfs[list(dfs.keys())[0]]

        return df_train, df_tune, df_test</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="rulevetting.projects.iai_pecarn.dataset.Dataset" href="../projects/iai_pecarn/dataset.html#rulevetting.projects.iai_pecarn.dataset.Dataset">Dataset</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="rulevetting.templates.dataset.DatasetTemplate.clean_data"><code class="name flex">
<span>def <span class="ident">clean_data</span></span>(<span>self, data_path: str = '/Volumes/GoogleDrive/My Drive/research/rules/rule-vetting/data', **kwargs) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Convert the raw data files into a pandas dataframe.
Dataframe keys should be reasonable (lowercase, underscore-separated).
Data types should be reasonable.</p>
<h2 id="params">Params</h2>
<p>data_path: str, optional
Path to all data files
kwargs: dict
Dictionary of hyperparameters specifying judgement calls</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>cleaned_data</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def clean_data(self, data_path: str = rulevetting.DATA_PATH, **kwargs) -&gt; pd.DataFrame:
    &#34;&#34;&#34;
    Convert the raw data files into a pandas dataframe.
    Dataframe keys should be reasonable (lowercase, underscore-separated).
    Data types should be reasonable.

    Params
    ------
    data_path: str, optional
        Path to all data files
    kwargs: dict
        Dictionary of hyperparameters specifying judgement calls

    Returns
    -------
    cleaned_data: pd.DataFrame
    &#34;&#34;&#34;
    return NotImplemented</code></pre>
</details>
</dd>
<dt id="rulevetting.templates.dataset.DatasetTemplate.extract_features"><code class="name flex">
<span>def <span class="ident">extract_features</span></span>(<span>self, preprocessed_data: pandas.core.frame.DataFrame, **kwargs) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Extract features from preprocessed data
All features should be binary</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>preprocessed_data</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary of hyperparameters specifying judgement calls</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>extracted_features</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def extract_features(self, preprocessed_data: pd.DataFrame, **kwargs) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Extract features from preprocessed data
    All features should be binary


    Parameters
    ----------
    preprocessed_data: pd.DataFrame
    kwargs: dict
        Dictionary of hyperparameters specifying judgement calls

    Returns
    -------
    extracted_features: pd.DataFrame
    &#34;&#34;&#34;
    return NotImplemented</code></pre>
</details>
</dd>
<dt id="rulevetting.templates.dataset.DatasetTemplate.get_data"><code class="name flex">
<span>def <span class="ident">get_data</span></span>(<span>self, save_csvs: bool = False, data_path: str = '/Volumes/GoogleDrive/My Drive/research/rules/rule-vetting/data', load_csvs: bool = False, run_perturbations: bool = False) ‑> (<class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.frame.DataFrame'>)</span>
</code></dt>
<dd>
<div class="desc"><p>Runs all the processing and returns the data.
This method does not need to be overriden.</p>
<h2 id="params">Params</h2>
<p>save_csvs: bool, optional
Whether to save csv files of the processed data
data_path: str, optional
Path to all data
load_csvs: bool, optional
Whether to skip all processing and load data directly from csvs
run_perturbations: bool, optional
Whether to run / save data pipeline for all combinations of judgement calls</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>df_train</code></dt>
<dd>&nbsp;</dd>
<dt><code>df_tune</code></dt>
<dd>&nbsp;</dd>
<dt><code>df_test</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_data(self, save_csvs: bool = False,
             data_path: str = rulevetting.DATA_PATH,
             load_csvs: bool = False,
             run_perturbations: bool = False) -&gt; (pd.DataFrame, pd.DataFrame, pd.DataFrame):
    &#34;&#34;&#34;Runs all the processing and returns the data.
    This method does not need to be overriden.

    Params
    ------
    save_csvs: bool, optional
        Whether to save csv files of the processed data
    data_path: str, optional
        Path to all data
    load_csvs: bool, optional
        Whether to skip all processing and load data directly from csvs
    run_perturbations: bool, optional
        Whether to run / save data pipeline for all combinations of judgement calls

    Returns
    -------
    df_train
    df_tune
    df_test
    &#34;&#34;&#34;
    PROCESSED_PATH = oj(data_path, self.get_dataset_id(), &#39;processed&#39;)
    if load_csvs:
        return tuple([pd.read_csv(oj(PROCESSED_PATH, s), index_col=0)
                      for s in [&#39;train.csv&#39;, &#39;tune.csv&#39;, &#39;test.csv&#39;]])
    np.random.seed(0)
    random.seed(0)
    CACHE_PATH = oj(data_path, &#39;joblib_cache&#39;)
    cache = Memory(CACHE_PATH, verbose=0).cache
    kwargs = self.get_judgement_calls_dictionary()
    default_kwargs = {}
    for key in kwargs.keys():
        func_kwargs = kwargs[key]
        default_kwargs[key] = {k: func_kwargs[k][0]  # first arg in each list is default
                               for k in func_kwargs.keys()}

    print(&#39;kwargs&#39;, default_kwargs)
    if not run_perturbations:
        cleaned_data = cache(self.clean_data)(data_path=data_path, **default_kwargs[&#39;clean_data&#39;])
        preprocessed_data = cache(self.preprocess_data)(cleaned_data, **default_kwargs[&#39;preprocess_data&#39;])
        extracted_features = cache(self.extract_features)(preprocessed_data, **default_kwargs[&#39;extract_features&#39;])
        df_train, df_tune, df_test = cache(self.split_data)(extracted_features)
    elif run_perturbations:
        data_path_arg = init_args([data_path], names=[&#39;data_path&#39;])[0]
        clean_set = build_Vset(&#39;clean_data&#39;, self.clean_data, param_dict=kwargs[&#39;clean_data&#39;], cache_dir=CACHE_PATH)
        cleaned_data = clean_set(data_path_arg)
        preprocess_set = build_Vset(&#39;preprocess_data&#39;, self.preprocess_data, param_dict=kwargs[&#39;preprocess_data&#39;],
                                    cache_dir=CACHE_PATH)
        preprocessed_data = preprocess_set(cleaned_data)
        extract_set = build_Vset(&#39;extract_features&#39;, self.extract_features, param_dict=kwargs[&#39;extract_features&#39;],
                                 cache_dir=CACHE_PATH)
        extracted_features = extract_set(preprocessed_data)
        split_data = Vset(&#39;split_data&#39;, modules=[self.split_data])
        dfs = split_data(extracted_features)
    if save_csvs:
        os.makedirs(PROCESSED_PATH, exist_ok=True)

        if not run_perturbations:
            for df, fname in zip([df_train, df_tune, df_test],
                                 [&#39;train.csv&#39;, &#39;tune.csv&#39;, &#39;test.csv&#39;]):
                meta_keys = rulevetting.api.util.get_feat_names_from_base_feats(df.keys(), self.get_meta_keys())
                df.loc[:, meta_keys].to_csv(oj(PROCESSED_PATH, f&#39;meta_{fname}&#39;))
                df.drop(columns=meta_keys).to_csv(oj(PROCESSED_PATH, fname))
        if run_perturbations:
            for k in dfs.keys():
                if isinstance(k, tuple):
                    os.makedirs(oj(PROCESSED_PATH, &#39;perturbed_data&#39;), exist_ok=True)
                    perturbation_name = str(k).replace(&#39;, &#39;, &#39;_&#39;).replace(&#39;(&#39;, &#39;&#39;).replace(&#39;)&#39;, &#39;&#39;)
                    perturbed_path = oj(PROCESSED_PATH, &#39;perturbed_data&#39;, perturbation_name)
                    os.makedirs(perturbed_path, exist_ok=True)
                    for i, fname in enumerate([&#39;train.csv&#39;, &#39;tune.csv&#39;, &#39;test.csv&#39;]):
                        df = dfs[k][i]
                        meta_keys = rulevetting.api.util.get_feat_names_from_base_feats(df.keys(),
                                                                                        self.get_meta_keys())
                        df.loc[:, meta_keys].to_csv(oj(perturbed_path, f&#39;meta_{fname}&#39;))
                        df.drop(columns=meta_keys).to_csv(oj(perturbed_path, fname))
            return dfs[list(dfs.keys())[0]]

    return df_train, df_tune, df_test</code></pre>
</details>
</dd>
<dt id="rulevetting.templates.dataset.DatasetTemplate.get_dataset_id"><code class="name flex">
<span>def <span class="ident">get_dataset_id</span></span>(<span>self) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Should return the name of the dataset id (str)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def get_dataset_id(self) -&gt; str:
    &#34;&#34;&#34;Should return the name of the dataset id (str)
    &#34;&#34;&#34;
    return NotImplemented</code></pre>
</details>
</dd>
<dt id="rulevetting.templates.dataset.DatasetTemplate.get_judgement_calls_dictionary"><code class="name flex">
<span>def <span class="ident">get_judgement_calls_dictionary</span></span>(<span>self) ‑> Dict[str, Dict[str, list]]</span>
</code></dt>
<dd>
<div class="desc"><p>Return dictionary of keyword arguments for each function in the dataset class.
Each key should be a string with the name of the arg.
Each value should be a list of values, with the default value coming first.</p>
<h2 id="example">Example</h2>
<p>return {
'clean_data': {},
'preprocess_data': {
'imputation_strategy': ['mean', 'median'],
# first value is default
},
'extract_features': {},
}</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_judgement_calls_dictionary(self) -&gt; Dict[str, Dict[str, list]]:
    &#34;&#34;&#34;Return dictionary of keyword arguments for each function in the dataset class.
    Each key should be a string with the name of the arg.
    Each value should be a list of values, with the default value coming first.

    Example
    -------
    return {
        &#39;clean_data&#39;: {},
        &#39;preprocess_data&#39;: {
            &#39;imputation_strategy&#39;: [&#39;mean&#39;, &#39;median&#39;],  # first value is default
        },
        &#39;extract_features&#39;: {},
    }
    &#34;&#34;&#34;
    return NotImplemented</code></pre>
</details>
</dd>
<dt id="rulevetting.templates.dataset.DatasetTemplate.get_meta_keys"><code class="name flex">
<span>def <span class="ident">get_meta_keys</span></span>(<span>self) ‑> list</span>
</code></dt>
<dd>
<div class="desc"><p>Return list of keys which should not be used in fitting but are still useful for analysis</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def get_meta_keys(self) -&gt; list:
    &#34;&#34;&#34;Return list of keys which should not be used in fitting but are still useful for analysis
    &#34;&#34;&#34;
    return NotImplemented</code></pre>
</details>
</dd>
<dt id="rulevetting.templates.dataset.DatasetTemplate.get_outcome_name"><code class="name flex">
<span>def <span class="ident">get_outcome_name</span></span>(<span>self) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Should return the name of the outcome we are predicting</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def get_outcome_name(self) -&gt; str:
    &#34;&#34;&#34;Should return the name of the outcome we are predicting
    &#34;&#34;&#34;
    return NotImplemented</code></pre>
</details>
</dd>
<dt id="rulevetting.templates.dataset.DatasetTemplate.preprocess_data"><code class="name flex">
<span>def <span class="ident">preprocess_data</span></span>(<span>self, cleaned_data: pandas.core.frame.DataFrame, **kwargs) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Preprocess the data.
Impute missing values.
Scale/transform values.
Should put the prediction target in a column named "outcome"</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>cleaned_data</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary of hyperparameters specifying judgement calls</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>preprocessed_data</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def preprocess_data(self, cleaned_data: pd.DataFrame, **kwargs) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Preprocess the data.
    Impute missing values.
    Scale/transform values.
    Should put the prediction target in a column named &#34;outcome&#34;

    Parameters
    ----------
    cleaned_data: pd.DataFrame
    kwargs: dict
        Dictionary of hyperparameters specifying judgement calls

    Returns
    -------
    preprocessed_data: pd.DataFrame
    &#34;&#34;&#34;
    return NotImplemented</code></pre>
</details>
</dd>
<dt id="rulevetting.templates.dataset.DatasetTemplate.split_data"><code class="name flex">
<span>def <span class="ident">split_data</span></span>(<span>self, preprocessed_data: pandas.core.frame.DataFrame) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Split into 3 sets: training, tuning, testing.
Do not modify (to ensure consistent test set).
Keep in mind any natural splits (e.g. hospitals).
Ensure that there are positive points in all splits.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>preprocessed_data</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary of hyperparameters specifying judgement calls</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>df_train</code></dt>
<dd>&nbsp;</dd>
<dt><code>df_tune</code></dt>
<dd>&nbsp;</dd>
<dt><code>df_test</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def split_data(self, preprocessed_data: pd.DataFrame) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Split into 3 sets: training, tuning, testing.
    Do not modify (to ensure consistent test set).
    Keep in mind any natural splits (e.g. hospitals).
    Ensure that there are positive points in all splits.

    Parameters
    ----------
    preprocessed_data
    kwargs: dict
        Dictionary of hyperparameters specifying judgement calls

    Returns
    -------
    df_train
    df_tune
    df_test
    &#34;&#34;&#34;
    return tuple(np.split(
        preprocessed_data.sample(frac=1, random_state=42),
        [int(.6 * len(preprocessed_data)),  # 60% train
         int(.8 * len(preprocessed_data))]  # 20% tune, 20% test
    ))</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="rulevetting.templates" href="index.html">rulevetting.templates</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="rulevetting.templates.dataset.DatasetTemplate" href="#rulevetting.templates.dataset.DatasetTemplate">DatasetTemplate</a></code></h4>
<ul class="">
<li><code><a title="rulevetting.templates.dataset.DatasetTemplate.clean_data" href="#rulevetting.templates.dataset.DatasetTemplate.clean_data">clean_data</a></code></li>
<li><code><a title="rulevetting.templates.dataset.DatasetTemplate.extract_features" href="#rulevetting.templates.dataset.DatasetTemplate.extract_features">extract_features</a></code></li>
<li><code><a title="rulevetting.templates.dataset.DatasetTemplate.get_data" href="#rulevetting.templates.dataset.DatasetTemplate.get_data">get_data</a></code></li>
<li><code><a title="rulevetting.templates.dataset.DatasetTemplate.get_dataset_id" href="#rulevetting.templates.dataset.DatasetTemplate.get_dataset_id">get_dataset_id</a></code></li>
<li><code><a title="rulevetting.templates.dataset.DatasetTemplate.get_judgement_calls_dictionary" href="#rulevetting.templates.dataset.DatasetTemplate.get_judgement_calls_dictionary">get_judgement_calls_dictionary</a></code></li>
<li><code><a title="rulevetting.templates.dataset.DatasetTemplate.get_meta_keys" href="#rulevetting.templates.dataset.DatasetTemplate.get_meta_keys">get_meta_keys</a></code></li>
<li><code><a title="rulevetting.templates.dataset.DatasetTemplate.get_outcome_name" href="#rulevetting.templates.dataset.DatasetTemplate.get_outcome_name">get_outcome_name</a></code></li>
<li><code><a title="rulevetting.templates.dataset.DatasetTemplate.preprocess_data" href="#rulevetting.templates.dataset.DatasetTemplate.preprocess_data">preprocess_data</a></code></li>
<li><code><a title="rulevetting.templates.dataset.DatasetTemplate.split_data" href="#rulevetting.templates.dataset.DatasetTemplate.split_data">split_data</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>